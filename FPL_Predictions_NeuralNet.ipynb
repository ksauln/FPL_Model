{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6548a3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 13:34:04.387462: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import linprog\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66beb5a1",
   "metadata": {},
   "source": [
    "# Load and Combine Seasons\n",
    "\n",
    "1. **Base URL and Seasons**: It defines a base URL to access raw data from a GitHub repository and specifies the seasons of interest (2024-25, 2023-24, 2022-23, 2021-22).\n",
    "\n",
    "2. **Model Name**: It sets a model name (`MODEL_NAME`) that presumably refers to a pre-trained model file.\n",
    "\n",
    "3. **Loading Season Data**: The function `load_season_data(season)` performs the following tasks:\n",
    "   - Initializes a dictionary `data` to store data for various categories (players, player IDs, fixtures, teams, and player history).\n",
    "   - Defines a dictionary of file names corresponding to each category.\n",
    "   - Iterates over the file names, reading the CSV files from the specified season's folder using the base URL. \n",
    "   - For the \"teams\" category, it only loads specific columns (`id` and `name`).\n",
    "   - Adds a new column, `season`, to each dataframe to indicate the season from which the data is sourced.\n",
    "\n",
    "4. **Loading All Seasons Data**: The function `load_all_seasons_data(seasons)`:\n",
    "   - Initializes a dictionary `all_data` to hold lists for each data category.\n",
    "   - Calls `load_season_data(season)` for each specified season to load the data.\n",
    "   - Appends the data from each season to the respective lists in `all_data`.\n",
    "   - Finally, concatenates the lists into a single dataframe for each category, ignoring index conflicts.\n",
    "\n",
    "5. **Combining Data**: After defining the functions, the code calls `load_all_seasons_data(seasons)` to load and combine data from all specified seasons.\n",
    "\n",
    "6. **Accessing Combined Data**: It stores the combined dataframes for players, fixtures, player IDs, player history, and teams in separate variables for further analysis or processing.\n",
    "\n",
    "In summary, the section fetches and organizes fantasy football data from multiple seasons into structured dataframes for subsequent use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "425fb9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://raw.githubusercontent.com/vaastav/Fantasy-Premier-League/master/data/'\n",
    "seasons = ['2024-25', '2023-24', '2022-23', '2021-22']\n",
    "MODEL_NAME = 'FPL_NeuralNet_20241017.joblib'\n",
    "SCALER_FILENAME = 'FPL_NeuralNet_scaler.pkl'\n",
    "\n",
    "def load_season_data(season):\n",
    "    data = {}\n",
    "    files = {\n",
    "        'players': 'players_raw.csv',\n",
    "        'player_ids': 'player_idlist.csv',\n",
    "        'fixtures': 'fixtures.csv',\n",
    "        'teams': 'teams.csv',\n",
    "        'player_history': 'gws/merged_gw.csv'\n",
    "    }\n",
    "    \n",
    "    for key, file in files.items():\n",
    "        if key == 'teams':\n",
    "            data[key] = pd.read_csv(f'{base_url}{season}/{file}', usecols=['id', 'name'])\n",
    "        else:\n",
    "            data[key] = pd.read_csv(f'{base_url}{season}/{file}')\n",
    "        \n",
    "        data[key]['season'] = season\n",
    "    \n",
    "    return data\n",
    "\n",
    "def load_all_seasons_data(seasons):\n",
    "    all_data = {key: [] for key in ['players', 'player_ids', 'fixtures', 'teams', 'player_history']}\n",
    "    \n",
    "    for season in seasons:\n",
    "        season_data = load_season_data(season)\n",
    "        for key in all_data:\n",
    "            all_data[key].append(season_data[key])\n",
    "    \n",
    "    return {key: pd.concat(value, ignore_index=True) for key, value in all_data.items()}\n",
    "\n",
    "# Load and combine data for all seasons\n",
    "combined_data = load_all_seasons_data(seasons)\n",
    "\n",
    "# Access the combined dataframes\n",
    "players = combined_data['players']\n",
    "fixtures = combined_data['fixtures']\n",
    "player_ids = combined_data['player_ids']\n",
    "player_history = combined_data['player_history']\n",
    "teams = combined_data['teams']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05b5e58",
   "metadata": {},
   "source": [
    "# Clean Data and Prep for Model\n",
    "\n",
    "### 1. **Creating Unique IDs**\n",
    "- **Function `create_unique_ids(player_ids, teams)`**: \n",
    "  - Generates unique identifiers for players and teams.\n",
    "  - Combines player first and second names to create a full name and drops duplicates to create a unique player dataframe (`players_unique`).\n",
    "  - Creates a unique ID for each player based on their index.\n",
    "  - Drops duplicates from the teams dataframe and creates a unique ID for each team.\n",
    "\n",
    "### 2. **Merging Unique IDs**\n",
    "- **Function `merge_unique_ids(players, player_history, teams, players_unique, teams_unique, fixtures)`**: \n",
    "  - Merges unique player IDs into player data and player history based on names.\n",
    "  - Merges unique team IDs into team data and renames columns to better reflect the data.\n",
    "  - Merges team IDs into players and player history based on their respective teams for the season.\n",
    "  - Adds unique team IDs to fixtures and creates a combined `game` identifier for easier analysis.\n",
    "\n",
    "### 3. **Cleaning Player Data**\n",
    "- **Function `clean_players(df)`**: \n",
    "  - Cleans player data by converting certain columns to numeric types, filling missing values, and dropping unnecessary columns.\n",
    "  - Renames the full name column for consistency.\n",
    "\n",
    "### 4. **Cleaning Player History Data**\n",
    "- **Function `clean_player_history(df)`**: \n",
    "  - Cleans player history data by dropping rows without unique player IDs and filling missing values in key columns.\n",
    "  - Converts columns to appropriate data types and calculates cumulative statistics (points, minutes) and averages for performance metrics.\n",
    "\n",
    "### 5. **Formatting Columns**\n",
    "- **Function `format_columns(df)`**: \n",
    "  - Converts specific columns to strings or numeric types and rounds numeric values for consistency.\n",
    "\n",
    "### 6. **Calculating Rolling Averages**\n",
    "- **Functions `calculate_overall_rolling_averages(df, window=5)` and `calculate_opponent_rolling_difficulty(df, window=5)`**: \n",
    "  - Calculate rolling averages for goals scored and conceded for teams over a specified window (default is 5).\n",
    "  - Determine opponent difficulty by calculating rolling averages of team difficulties.\n",
    "\n",
    "### 7. **Merging Rolling Averages**\n",
    "- **Function `merge_rolling_avgs(df, overall_averages, fixture_difficulty)`**: \n",
    "  - Merges calculated rolling averages and difficulty metrics into the player history dataframe for comprehensive analysis.\n",
    "\n",
    "### 8. **Main Processing Function**\n",
    "- **Function `process_data(player_ids, teams, players, player_history, fixtures)`**: \n",
    "  - Integrates all previous functions to clean and process data.\n",
    "  - Creates unique IDs, merges them, cleans player and player history data, calculates rolling averages, and ensures that required columns are present in the final output.\n",
    "  - Returns the processed dataframes for players, player history, teams, fixtures, unique IDs, required columns, overall averages, and fixture difficulty.\n",
    "\n",
    "### 9. **Usage**\n",
    "- The final comment shows how to call the `process_data` function with the relevant dataframes to perform the entire data processing workflow.\n",
    "\n",
    "Overall, this section efficiently organizes and prepares the fantasy football data for analysis by generating unique identifiers, cleaning data, calculating performance metrics, and ensuring that all necessary information is readily available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "343f09e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unique_ids(player_ids, teams):\n",
    "    # Create unique ID for each player\n",
    "    player_ids['name'] = player_ids['first_name'] + ' ' + player_ids['second_name']\n",
    "    players_unique = player_ids.drop(['id', 'season'], axis=1).drop_duplicates()\n",
    "    players_unique['PlayerUniqueID'] = players_unique.index\n",
    "\n",
    "    # Create unique ID for each team\n",
    "    teams_unique = teams.drop(['id', 'season'], axis=1).drop_duplicates().reset_index(drop=True)\n",
    "    teams_unique['TeamUniqueID'] = teams_unique.index + 1\n",
    "\n",
    "    return players_unique, teams_unique\n",
    "\n",
    "def merge_unique_ids(players, player_history, teams, players_unique, teams_unique, fixtures):\n",
    "    # Merge player unique IDs\n",
    "    players = players.merge(players_unique, on=['first_name', 'second_name'], how='left')\n",
    "    player_history = player_history.merge(players_unique, on='name', how='left')\n",
    "\n",
    "    # Merge team unique IDs\n",
    "    teams = teams.merge(teams_unique, on='name', how='left')\n",
    "    teams.rename(columns={'name': 'team_name'}, inplace=True)\n",
    "    teams_opponent = teams.rename(columns={'team_name': 'team_name_oppo', 'TeamUniqueID': 'TeamUniqueID_oppo'})\n",
    "\n",
    "    # Merge team IDs to players and player_history\n",
    "    players = players.merge(teams, left_on=['team', 'season'], right_on=['id', 'season'], how='left')\n",
    "    player_history = player_history.merge(teams, left_on=['team', 'season'], right_on=['team_name', 'season'], how='left')\n",
    "    player_history = player_history.merge(teams_opponent, left_on=['opponent_team', 'season'], right_on=['id', 'season'], how='left')\n",
    "    player_history.drop(['id_x', 'id_y'], axis=1, inplace=True)\n",
    "    \n",
    "    #add unique team ids to fixtures\n",
    "    fixtures = fixtures.merge(teams, left_on=['team_h','season'], right_on=['id','season'], how='left')\n",
    "    fixtures = fixtures.merge(teams_opponent, left_on=['team_a','season'], right_on=['id','season'], how='left')\n",
    "    fixtures.drop(['id_x','id_y'],axis = 1,inplace=True)\n",
    "\n",
    "    #add in column for season and gameweek\n",
    "    fixtures['game']=fixtures['season'].str.replace('-','') + fixtures['event'].astype(str).str.zfill(2)\n",
    "\n",
    "    return players, player_history, teams, fixtures\n",
    "\n",
    "def clean_players(df):\n",
    "    replace_cols = ['chance_of_playing_next_round', 'chance_of_playing_this_round', 'corners_and_indirect_freekicks_order',\n",
    "                    'direct_freekicks_order', 'penalties_order', 'clean_sheets_per_90', 'expected_assists',\n",
    "                    'expected_assists_per_90', 'expected_goal_involvements', 'expected_goal_involvements_per_90',\n",
    "                    'expected_goals', 'expected_goals_conceded', 'expected_goals_conceded_per_90', 'expected_goals_per_90',\n",
    "                    'form_rank', 'form_rank_type', 'goals_conceded_per_90', 'now_cost_rank', 'now_cost_rank_type',\n",
    "                    'points_per_game_rank', 'points_per_game_rank_type', 'saves_per_90', 'selected_rank', 'selected_rank_type',\n",
    "                    'starts', 'starts_per_90']\n",
    "\n",
    "    drop_cols = ['corners_and_indirect_freekicks_text', 'direct_freekicks_text', 'news', 'news_added', 'penalties_text',\n",
    "                 'photo', 'special', 'squad_number']\n",
    "\n",
    "    df['form'] = df['form'].astype(float)\n",
    "    df['total_points'] = df['total_points'].astype(float)\n",
    "    df['minutes'] = df['minutes'].astype(float)\n",
    "    df['cost'] = df['now_cost'] / 10\n",
    "    df['name'] = df['first_name'] + ' ' + df['second_name']\n",
    "\n",
    "    df[replace_cols] = df[replace_cols].fillna(0).replace('None', 0)\n",
    "    df[replace_cols] = df[replace_cols].apply(pd.to_numeric, errors='coerce')\n",
    "    df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_player_history(df):\n",
    "    df.dropna(subset=[\"PlayerUniqueID\"], inplace=True)\n",
    "    \n",
    "    replace_cols = ['expected_assists', 'expected_goal_involvements', 'expected_goals', 'expected_goals_conceded', 'starts']\n",
    "    df[replace_cols] = df[replace_cols].fillna(0)\n",
    "\n",
    "    for col in ['influence', 'creativity', 'threat', 'ict_index']:\n",
    "        df[col] = df[col].astype(float)\n",
    "\n",
    "    df['cost'] = df['value'] / 10\n",
    "    df['game'] = df['season'].str.replace('-', '') + df['GW'].astype(str).str.zfill(2)\n",
    "    df['was_home'] = df['was_home'].astype(int)\n",
    "\n",
    "    df = df.sort_values(by=['PlayerUniqueID', 'season', 'GW'])\n",
    "    df['cumulative_points'] = df.groupby(['PlayerUniqueID', 'season'])['total_points'].cumsum()\n",
    "    df['cumulative_minutes'] = df.groupby(['PlayerUniqueID', 'season'])['minutes'].cumsum()\n",
    "    df['ppm'] = (df['cumulative_points'] / df['cumulative_minutes']).fillna(0).replace([np.inf, -np.inf], 0).round(5)\n",
    "    df['points_per_cost'] = (df['cumulative_points'] / df['cost']).round(5)\n",
    "    df['rolling_avg_points'] = df.groupby('PlayerUniqueID')['total_points'].rolling(window=5, min_periods=1).mean().reset_index(level=0, drop=True).round(5)\n",
    "\n",
    "    df['position'] = df['position'].map({'GK': 1, 'DEF': 2, 'MID': 3, 'FWD': 4})\n",
    "    df.dropna(subset=['position'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def format_columns(df):\n",
    "    strings = ['position', 'PlayerUniqueID', 'TeamUniqueID', 'TeamUniqueID_oppo', 'game']\n",
    "    nums = ['xP', 'assists', 'clean_sheets', 'creativity', 'expected_assists',\n",
    "            'expected_goal_involvements', 'expected_goals', 'expected_goals_conceded', 'goals_conceded',\n",
    "            'goals_scored', 'influence', 'minutes', 'own_goals', 'penalties_missed', 'penalties_saved',\n",
    "            'red_cards', 'saves', 'selected', 'starts', 'threat', 'transfers_balance', 'cost', 'was_home',\n",
    "            'yellow_cards', 'cumulative_points', 'cumulative_minutes', 'ppm', 'rolling_avg_points',\n",
    "            'rolling_avg_goals_scored', 'rolling_avg_goals_conceded', 'rolling_team_difficulty']\n",
    "\n",
    "    df[strings] = df[strings].astype(str)\n",
    "    df[nums] = df[nums].round(5)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_overall_rolling_averages(df, window=5):\n",
    "    team_data = pd.concat([\n",
    "        df[['game', 'TeamUniqueID', 'team_h_score', 'team_a_score']].rename(columns={'TeamUniqueID': 'team_id', 'team_h_score': 'goals_scored', 'team_a_score': 'goals_conceded'}),\n",
    "        df[['game', 'TeamUniqueID_oppo', 'team_a_score', 'team_h_score']].rename(columns={'TeamUniqueID_oppo': 'team_id', 'team_a_score': 'goals_scored', 'team_h_score': 'goals_conceded'})\n",
    "    ])\n",
    "    \n",
    "    team_data = team_data.sort_values(['team_id', 'game'])\n",
    "    \n",
    "    team_data['rolling_avg_goals_scored'] = team_data.groupby('team_id')['goals_scored'].rolling(window=window, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    team_data['rolling_avg_goals_conceded'] = team_data.groupby('team_id')['goals_conceded'].rolling(window=window, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    return team_data\n",
    "\n",
    "def calculate_opponent_rolling_difficulty(df, window=5):\n",
    "    team_data_diff = pd.concat([\n",
    "        df[['game', 'TeamUniqueID', 'team_h_difficulty']].rename(columns={'TeamUniqueID': 'team_id', 'team_h_difficulty': 'team_difficulty'}),\n",
    "        df[['game', 'TeamUniqueID_oppo', 'team_a_difficulty']].rename(columns={'TeamUniqueID_oppo': 'team_id', 'team_a_difficulty': 'team_difficulty'})\n",
    "    ])\n",
    "    \n",
    "    team_data_diff = team_data_diff.sort_values(['team_id', 'game'])\n",
    "    team_data_diff['rolling_team_difficulty'] = team_data_diff.groupby('team_id')['team_difficulty'].rolling(window=window, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    return team_data_diff\n",
    "\n",
    "def merge_rolling_avgs(df, overall_averages, fixture_difficulty):\n",
    "    df = pd.merge(df, overall_averages[['game', 'team_id', 'rolling_avg_goals_scored', 'rolling_avg_goals_conceded']],\n",
    "                  left_on=['game', 'TeamUniqueID'], right_on=['game', 'team_id'], how='left')\n",
    "    df.drop(['team_id'], axis=1, inplace=True)\n",
    "\n",
    "    df = pd.merge(df, fixture_difficulty[['game', 'team_id', 'team_difficulty', 'rolling_team_difficulty']],\n",
    "                  left_on=['game', 'TeamUniqueID'], right_on=['game', 'team_id'], how='left')\n",
    "    df.drop(['team_id'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_data(player_ids, teams, players, player_history, fixtures):\n",
    "    players_unique, teams_unique = create_unique_ids(player_ids, teams)\n",
    "    players, player_history, teams, fixtures = merge_unique_ids(players, player_history, teams, players_unique, teams_unique, fixtures)\n",
    "    \n",
    "    players = clean_players(players)\n",
    "    player_history = clean_player_history(player_history)\n",
    "    #clean_player_history function has things in it out of order to the rolling and merge functions\n",
    "    \n",
    "    overall_averages = calculate_overall_rolling_averages(fixtures)\n",
    "    fixture_difficulty = calculate_opponent_rolling_difficulty(fixtures)\n",
    "    \n",
    "    player_history = merge_rolling_avgs(player_history, overall_averages, fixture_difficulty)\n",
    "    \n",
    "    player_history=format_columns(player_history)\n",
    "    \n",
    "    player_history['TeamUniqueID'] = player_history['TeamUniqueID'].astype(str)\n",
    "    players['TeamUniqueID'] = players['TeamUniqueID'].astype(str)\n",
    "    \n",
    "    required_columns = [\n",
    "        'position', 'xP', 'assists', 'clean_sheets', 'creativity', 'expected_assists',\n",
    "        'expected_goal_involvements', 'expected_goals', 'expected_goals_conceded', 'goals_conceded', \n",
    "        'goals_scored', 'influence', 'minutes', 'own_goals', 'penalties_missed', 'penalties_saved', \n",
    "        'red_cards', 'saves', 'starts', 'threat', 'transfers_balance', 'cost', 'was_home',\n",
    "        'yellow_cards', 'PlayerUniqueID', 'TeamUniqueID', 'TeamUniqueID_oppo', 'cumulative_points',\n",
    "        'cumulative_minutes', 'ppm', 'rolling_avg_points', 'rolling_avg_goals_scored', \n",
    "        'rolling_avg_goals_conceded', 'rolling_team_difficulty', 'game'\n",
    "    ]\n",
    "    \n",
    "    for col in required_columns:\n",
    "        if col not in player_history.columns:\n",
    "            player_history[col] = np.nan\n",
    "    #some of these columns are exported for use later on\n",
    "    return players, player_history, teams, fixtures, players_unique, required_columns, overall_averages, fixture_difficulty\n",
    "\n",
    "# Usage\n",
    "# players, player_history, teams, fixtures = process_data(player_ids, teams, players, player_history, fixtures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2d65c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "players, player_history, teams, fixtures, players_unique, required_columns, overall_averages, fixture_difficulty = process_data(player_ids, teams, players, player_history, fixtures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d820a6f",
   "metadata": {},
   "source": [
    "# Run The Model\n",
    "\n",
    "This code is designed to train and update a machine learning model that predicts player performance across game weeks, specifically for fantasy football, using player history data. Here's a breakdown of what each function does:\n",
    "\n",
    "1. **Saving and Loading Model/Scaler**:\n",
    "   - `save_model_and_scaler`: Saves a trained model and a scaler using `joblib`.\n",
    "   - `load_model_and_scaler`: Loads a previously saved model and scaler. Handles file-not-found errors.\n",
    "\n",
    "2. **Data Preparation**:\n",
    "   - `prepare_data`: Prepares the player data by imputing missing numeric values, converting categorical columns, and ensuring columns like 'game' and 'position' are ordered.\n",
    "   - `add_season_weights`: Adds a weight to each player's data based on how recent the season is, allowing for older seasons to have less influence in the model.\n",
    "\n",
    "3. **Preprocessing**:\n",
    "   - `preprocess_data`: Scales the feature data using `StandardScaler` and converts training labels and sample weights to numpy arrays for compatibility with TensorFlow.\n",
    "\n",
    "4. **Model Creation**:\n",
    "   - `create_model`: Creates a neural network using Keras with customizable parameters such as learning rate, number of neurons, layers, and dropout. The model is compiled with weighted metrics (`MAE`, `MSE`).\n",
    "\n",
    "5. **Model Evaluation**:\n",
    "   - `evaluate_model`: Evaluates the model on test data, calculates `MAE` and `R2`, and returns the results.\n",
    "\n",
    "6. **Model Training**:\n",
    "   - `train_nn_model_with_grid_search`: Trains a neural network using grid search over basic parameter settings. Implements early stopping to prevent overfitting.\n",
    "\n",
    "7. **Dynamic Training Loop**:\n",
    "   - `dynamic_model_training_with_updates`: Automates the process of iteratively training the model with new game data. It loads the latest model, preprocesses the data, trains or updates the model, and evaluates predictions for each game week. The model and scaler are saved after each game week.\n",
    "\n",
    "8. **Model Update**:\n",
    "   - `update_model`: Retrains the model on new data, ensuring the features are scaled correctly and optionally using sample weights.\n",
    "\n",
    "9. **Usage**:\n",
    "   - The main part of the script defines features, target, and current season games. It calls `dynamic_model_training_with_updates` to train the model on player history data across multiple game weeks and make predictions.\n",
    "\n",
    "### Overall Workflow:\n",
    "The section prepares player data, trains or updates a neural network for game outcomes, evaluates its performance, and saves the model for future use. Each game week is processed in a loop, allowing for dynamic updates to the model based on the most recent data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11a67057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_scaler(model, scaler, model_name, scaler_filename):\n",
    "    # Save the model\n",
    "    joblib.dump(model, model_name)\n",
    "    print(f\"Model saved as {model_name}\")\n",
    "    \n",
    "    # Save the scaler\n",
    "    joblib.dump(scaler, scaler_filename)\n",
    "    print(f\"Scaler saved as {scaler_filename}\")\n",
    "\n",
    "def load_model_and_scaler(model_name, scaler_filename):\n",
    "    try:\n",
    "        # Load the model\n",
    "        model = joblib.load(model_name)\n",
    "        print(f\"Model loaded from {model_name}\")\n",
    "        \n",
    "        # Load the scaler\n",
    "        scaler = joblib.load(scaler_filename)\n",
    "        print(f\"Scaler loaded from {scaler_filename}\")\n",
    "        \n",
    "        return model, scaler\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def prepare_data(player_history, features, target):\n",
    "    #print(f\"Shape of player_history before prepare_data: {player_history.shape}\")\n",
    "    \n",
    "    # Identify numeric and categorical features\n",
    "    numeric_features = player_history[features].select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = player_history[features].select_dtypes(include=[object]).columns.tolist()\n",
    "\n",
    "    # Impute numeric features\n",
    "    numeric_imputer = SimpleImputer(strategy='mean')\n",
    "    player_history[numeric_features] = numeric_imputer.fit_transform(player_history[numeric_features])\n",
    "\n",
    "    # Convert object columns to 'category' dtype\n",
    "    for col in categorical_features:\n",
    "        player_history[col] = player_history[col].astype('category')\n",
    "        \n",
    "    # Ensure 'game' column is ordered\n",
    "    if 'game' in player_history.columns:\n",
    "        player_history['game'] = pd.Categorical(player_history['game'], ordered=True)\n",
    "\n",
    "    # Ensure 'position' column is ordered if applicable\n",
    "#     if 'position' in player_history.columns:\n",
    "#         player_history['position'] = pd.Categorical(player_history['position'], \n",
    "#                                                     categories=['GK', 'DEF', 'MID', 'FWD'], \n",
    "#                                                     ordered=True)\n",
    "    \n",
    "    #print(f\"Shape of player_history after prepare_data: {player_history.shape}\")\n",
    "    return player_history\n",
    "\n",
    "def add_season_weights(player_history):\n",
    "    #print(f\"Shape of player_history before add_season_weights: {player_history.shape}\")\n",
    "    \n",
    "    # Extract the year from the 'season' column\n",
    "    player_history['season'] = player_history['season'].str.split('-').str[0]\n",
    "\n",
    "    # Convert 'season' to numeric\n",
    "    player_history['season'] = pd.to_numeric(player_history['season'], errors='coerce')\n",
    "    \n",
    "    # Drop any rows where 'season' is NaN\n",
    "    player_history = player_history.dropna(subset=['season'])\n",
    "    \n",
    "    current_season = player_history['season'].max()\n",
    "    min_season = player_history['season'].min()\n",
    "    \n",
    "    player_history['season_weight'] = (player_history['season'] - min_season + 1) / (current_season - min_season + 1)\n",
    "    \n",
    "    #print(f\"Shape of player_history after add_season_weights: {player_history.shape}\")\n",
    "    return player_history\n",
    "\n",
    "def preprocess_data(X_train, y_train, sample_weights=None):\n",
    "    \"\"\"\n",
    "    Preprocess the data to ensure compatibility with TensorFlow.\n",
    "    \"\"\"\n",
    "    # Convert y_train and sample_weights to numpy arrays\n",
    "    y_train = np.array(y_train, dtype=np.float32)\n",
    "    \n",
    "    if sample_weights is not None:\n",
    "        sample_weights = np.array(sample_weights, dtype=np.float32)\n",
    "    \n",
    "    # Scale the features using the DataFrame directly to keep column names\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    \n",
    "    return X_train_scaled, y_train, sample_weights, scaler\n",
    "\n",
    "def create_model(input_dim, learning_rate=0.001, neurons=64, layers=2, dropout_rate=0.2, activation='relu'):\n",
    "    \"\"\"Create and compile the neural network model with weighted metrics\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(Dense(neurons, input_dim=input_dim, activation=activation))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for _ in range(layers - 1):\n",
    "        model.add(Dense(neurons, activation=activation))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    # Compile with weighted metrics explicitly specified\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mse',\n",
    "        metrics=['mae', 'mse'],  # Regular metrics\n",
    "        weighted_metrics=['mae', 'mse']  # Specify which metrics should be weighted\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, sample_weights=None):\n",
    "    \"\"\"Evaluate model performance using weighted metrics\"\"\"\n",
    "    if sample_weights is not None:\n",
    "        evaluation = model.evaluate(\n",
    "            X_test, \n",
    "            y_test, \n",
    "            sample_weight=sample_weights,\n",
    "            verbose=0\n",
    "        )\n",
    "    else:\n",
    "        evaluation = model.evaluate(\n",
    "            X_test, \n",
    "            y_test, \n",
    "            verbose=0\n",
    "        )\n",
    "    \n",
    "    # Get predictions for R2 score calculation\n",
    "    predictions = model.predict(X_test, verbose=0)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    \n",
    "    # The first value is the loss, second is MAE, third is weighted MAE\n",
    "    mae = evaluation[1]  # Use the unweighted MAE\n",
    "    \n",
    "    return mae, r2\n",
    "\n",
    "def train_nn_model_with_grid_search(X_train, y_train, sample_weights=None):\n",
    "    \"\"\"\n",
    "    Train neural network model with basic parameter settings\n",
    "    \"\"\"\n",
    "    # Ensure all inputs are float32\n",
    "    X_train_scaled = np.array(X_train, dtype=np.float32)\n",
    "    y_train_processed = np.array(y_train, dtype=np.float32)\n",
    "    \n",
    "    if sample_weights is not None:\n",
    "        sample_weights = np.array(sample_weights, dtype=np.float32)\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_scaled)\n",
    "    \n",
    "    # Model parameters\n",
    "    params = {\n",
    "        'learning_rate': 0.001,\n",
    "        'neurons': 64,\n",
    "        'layers': 2,\n",
    "        'dropout_rate': 0.2,\n",
    "        'activation': 'relu'\n",
    "    }\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(\n",
    "        input_dim=X_train_scaled.shape[1],\n",
    "        **params\n",
    "    )\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Train the model\n",
    "        if sample_weights is not None:\n",
    "            history = model.fit(\n",
    "                X_train_scaled,\n",
    "                y_train_processed,\n",
    "                sample_weight=sample_weights,\n",
    "                validation_split=0.2,\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                callbacks=[early_stopping],\n",
    "                verbose=1\n",
    "            )\n",
    "        else:\n",
    "            history = model.fit(\n",
    "                X_train_scaled,\n",
    "                y_train_processed,\n",
    "                validation_split=0.2,\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                callbacks=[early_stopping],\n",
    "                verbose=1\n",
    "            )\n",
    "        \n",
    "        return model, scaler\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {str(e)}\")\n",
    "        raise\n",
    "        \n",
    "def dynamic_model_training_with_updates(player_history, features, target, current_season_games, model_name, scaler_filename):\n",
    "    player_history = prepare_data(player_history, features, target)\n",
    "    player_history = add_season_weights(player_history)\n",
    "    \n",
    "    # Load the latest model and scaler\n",
    "    latest_model, scaler = load_model_and_scaler(model_name, scaler_filename)\n",
    "    \n",
    "    all_predictions = []\n",
    "    maes = []\n",
    "    r2s = []\n",
    "    \n",
    "    # Loop through game weeks\n",
    "    for game in current_season_games:\n",
    "        print(f\"\\nProcessing game week: {game}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_data = player_history[player_history['game'] < game]\n",
    "        test_data = player_history[player_history['game'] == game]\n",
    "        \n",
    "        X_train = train_data[features]\n",
    "        y_train = train_data[target]\n",
    "        X_test = test_data[features]\n",
    "        y_test = test_data[target]\n",
    "        \n",
    "        sample_weights = train_data['season_weight'].values.astype(np.float32)\n",
    "        \n",
    "        # Train the model if no previous model exists\n",
    "        if latest_model is None:\n",
    "            print(\"Training new model...\")\n",
    "            model, scaler = train_nn_model_with_grid_search(X_train, y_train, sample_weights)\n",
    "            latest_model = model\n",
    "        else:\n",
    "            model = update_model(latest_model, X_train, y_train, X_test, y_test, scaler, sample_weights)\n",
    "            latest_model = model\n",
    "        \n",
    "        # Scale the test data\n",
    "        X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = model.predict(X_test_scaled, verbose=0)\n",
    "        \n",
    "        # Evaluate performance\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "        \n",
    "        maes.append(mae)\n",
    "        r2s.append(r2)\n",
    "        all_predictions.extend(predictions)\n",
    "        \n",
    "        player_history.loc[player_history['game'] == game, 'predicted_points'] = predictions.flatten()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"Training time: {int(elapsed_time // 60)} minutes and {elapsed_time % 60:.2f} seconds\")\n",
    "        print(f\"MAE: {mae:.4f}, R2: {r2:.4f}\")\n",
    "        \n",
    "        # Save the updated model and scaler\n",
    "        save_model_and_scaler(latest_model, scaler, model_name, scaler_filename)\n",
    "    \n",
    "    overall_mae = np.mean(maes)\n",
    "    overall_r2 = np.mean(r2s)\n",
    "    \n",
    "    print(f\"\\nOverall MAE: {overall_mae:.4f}\")\n",
    "    print(f\"Overall R2: {overall_r2:.4f}\")\n",
    "    \n",
    "    return all_predictions, overall_mae, overall_r2, latest_model\n",
    "\n",
    "def update_model(model, X_train, y_train, X_test, y_test, scaler, sample_weights=None):\n",
    "    \"\"\"Update existing model with new data\"\"\"\n",
    "    if scaler is not None:\n",
    "        X_train_scaled = scaler.transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_train_scaled = X_train\n",
    "        X_test_scaled = X_test\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        if sample_weights is not None:\n",
    "            model.fit(\n",
    "                X_train_scaled,\n",
    "                y_train,\n",
    "                sample_weight=sample_weights,\n",
    "                validation_data=(X_test_scaled, y_test),\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                callbacks=[early_stopping],\n",
    "                verbose=1\n",
    "            )\n",
    "        else:\n",
    "            model.fit(\n",
    "                X_train_scaled,\n",
    "                y_train,\n",
    "                validation_data=(X_test_scaled, y_test),\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                callbacks=[early_stopping],\n",
    "                verbose=1\n",
    "            )\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during model update: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a96d1009",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 10-17-2024 13:34:11\n",
      "File not found: [Errno 2] No such file or directory: 'FPL_NeuralNet_20241017.joblib'\n",
      "\n",
      "Processing game week: 20242501\n",
      "Training new model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 13:34:12.114197: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2752/2752 [==============================] - 6s 2ms/step - loss: 0.3486 - mae: 0.4354 - mse: 0.6858 - weighted_mae: 0.4179 - weighted_mse: 0.6465 - val_loss: 0.0474 - val_mae: 0.2035 - val_mse: 0.1592 - val_weighted_mae: 0.2006 - val_weighted_mse: 0.1530\n",
      "Epoch 2/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.1828 - mae: 0.3207 - mse: 0.3558 - weighted_mae: 0.3095 - weighted_mse: 0.3391 - val_loss: 0.0366 - val_mae: 0.1873 - val_mse: 0.1192 - val_weighted_mae: 0.1874 - val_weighted_mse: 0.1181\n",
      "Epoch 3/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.1568 - mae: 0.2956 - mse: 0.3043 - weighted_mae: 0.2867 - weighted_mse: 0.2907 - val_loss: 0.0345 - val_mae: 0.1745 - val_mse: 0.1160 - val_weighted_mae: 0.1725 - val_weighted_mse: 0.1114\n",
      "Epoch 4/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.1457 - mae: 0.2808 - mse: 0.2844 - weighted_mae: 0.2724 - weighted_mse: 0.2701 - val_loss: 0.0313 - val_mae: 0.2006 - val_mse: 0.1019 - val_weighted_mae: 0.1992 - val_weighted_mse: 0.1010\n",
      "Epoch 5/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.1336 - mae: 0.2697 - mse: 0.2608 - weighted_mae: 0.2619 - weighted_mse: 0.2478 - val_loss: 0.0278 - val_mae: 0.1751 - val_mse: 0.0925 - val_weighted_mae: 0.1728 - val_weighted_mse: 0.0898\n",
      "Epoch 6/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.1269 - mae: 0.2613 - mse: 0.2463 - weighted_mae: 0.2541 - weighted_mse: 0.2354 - val_loss: 0.0337 - val_mae: 0.1637 - val_mse: 0.1121 - val_weighted_mae: 0.1601 - val_weighted_mse: 0.1089\n",
      "Epoch 7/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.1256 - mae: 0.2579 - mse: 0.2446 - weighted_mae: 0.2507 - weighted_mse: 0.2330 - val_loss: 0.0317 - val_mae: 0.1754 - val_mse: 0.1079 - val_weighted_mae: 0.1697 - val_weighted_mse: 0.1022\n",
      "Epoch 8/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.1196 - mae: 0.2526 - mse: 0.2329 - weighted_mae: 0.2459 - weighted_mse: 0.2219 - val_loss: 0.0275 - val_mae: 0.1561 - val_mse: 0.0911 - val_weighted_mae: 0.1505 - val_weighted_mse: 0.0886\n",
      "Epoch 9/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.1147 - mae: 0.2485 - mse: 0.2257 - weighted_mae: 0.2413 - weighted_mse: 0.2126 - val_loss: 0.0243 - val_mae: 0.1259 - val_mse: 0.0808 - val_weighted_mae: 0.1223 - val_weighted_mse: 0.0783\n",
      "Epoch 10/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.1158 - mae: 0.2471 - mse: 0.2278 - weighted_mae: 0.2403 - weighted_mse: 0.2147 - val_loss: 0.0235 - val_mae: 0.1148 - val_mse: 0.0786 - val_weighted_mae: 0.1121 - val_weighted_mse: 0.0758\n",
      "Epoch 11/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.1149 - mae: 0.2432 - mse: 0.2243 - weighted_mae: 0.2367 - weighted_mse: 0.2130 - val_loss: 0.0245 - val_mae: 0.1243 - val_mse: 0.0807 - val_weighted_mae: 0.1224 - val_weighted_mse: 0.0790\n",
      "Epoch 12/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.1135 - mae: 0.2424 - mse: 0.2189 - weighted_mae: 0.2363 - weighted_mse: 0.2105 - val_loss: 0.0222 - val_mae: 0.1082 - val_mse: 0.0731 - val_weighted_mae: 0.1057 - val_weighted_mse: 0.0717\n",
      "Epoch 13/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.1110 - mae: 0.2396 - mse: 0.2163 - weighted_mae: 0.2333 - weighted_mse: 0.2058 - val_loss: 0.0228 - val_mae: 0.1054 - val_mse: 0.0754 - val_weighted_mae: 0.1035 - val_weighted_mse: 0.0736\n",
      "Epoch 14/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.1108 - mae: 0.2388 - mse: 0.2158 - weighted_mae: 0.2326 - weighted_mse: 0.2054 - val_loss: 0.0243 - val_mae: 0.1184 - val_mse: 0.0798 - val_weighted_mae: 0.1158 - val_weighted_mse: 0.0783\n",
      "Epoch 15/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.1069 - mae: 0.2369 - mse: 0.2096 - weighted_mae: 0.2308 - weighted_mse: 0.1982 - val_loss: 0.0224 - val_mae: 0.1077 - val_mse: 0.0728 - val_weighted_mae: 0.1056 - val_weighted_mse: 0.0721\n",
      "Epoch 16/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.1106 - mae: 0.2375 - mse: 0.2157 - weighted_mae: 0.2316 - weighted_mse: 0.2052 - val_loss: 0.0254 - val_mae: 0.1069 - val_mse: 0.0836 - val_weighted_mae: 0.1037 - val_weighted_mse: 0.0818\n",
      "Epoch 17/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.1068 - mae: 0.2343 - mse: 0.2085 - weighted_mae: 0.2281 - weighted_mse: 0.1980 - val_loss: 0.0288 - val_mae: 0.1287 - val_mse: 0.0946 - val_weighted_mae: 0.1260 - val_weighted_mse: 0.0930\n",
      "Epoch 18/50\n",
      "2752/2752 [==============================] - 3s 983us/step - loss: 0.1068 - mae: 0.2332 - mse: 0.2055 - weighted_mae: 0.2281 - weighted_mse: 0.1981 - val_loss: 0.0267 - val_mae: 0.1060 - val_mse: 0.0874 - val_weighted_mae: 0.1027 - val_weighted_mse: 0.0860\n",
      "Epoch 19/50\n",
      "2752/2752 [==============================] - 3s 939us/step - loss: 0.1050 - mae: 0.2332 - mse: 0.2044 - weighted_mae: 0.2275 - weighted_mse: 0.1947 - val_loss: 0.0212 - val_mae: 0.1086 - val_mse: 0.0700 - val_weighted_mae: 0.1061 - val_weighted_mse: 0.0683\n",
      "Epoch 20/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.1056 - mae: 0.2326 - mse: 0.2048 - weighted_mae: 0.2273 - weighted_mse: 0.1958 - val_loss: 0.0215 - val_mae: 0.1036 - val_mse: 0.0710 - val_weighted_mae: 0.1007 - val_weighted_mse: 0.0695\n",
      "Epoch 21/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.1012 - mae: 0.2293 - mse: 0.1970 - weighted_mae: 0.2237 - weighted_mse: 0.1876 - val_loss: 0.0221 - val_mae: 0.1214 - val_mse: 0.0735 - val_weighted_mae: 0.1197 - val_weighted_mse: 0.0712\n",
      "Epoch 22/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.0981 - mae: 0.2278 - mse: 0.1931 - weighted_mae: 0.2216 - weighted_mse: 0.1820 - val_loss: 0.0258 - val_mae: 0.0991 - val_mse: 0.0827 - val_weighted_mae: 0.0971 - val_weighted_mse: 0.0832\n",
      "Epoch 23/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.1034 - mae: 0.2302 - mse: 0.2003 - weighted_mae: 0.2245 - weighted_mse: 0.1917 - val_loss: 0.0241 - val_mae: 0.1007 - val_mse: 0.0794 - val_weighted_mae: 0.0983 - val_weighted_mse: 0.0776\n",
      "Epoch 24/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.1030 - mae: 0.2294 - mse: 0.2007 - weighted_mae: 0.2237 - weighted_mse: 0.1910 - val_loss: 0.0310 - val_mae: 0.1038 - val_mse: 0.1026 - val_weighted_mae: 0.1012 - val_weighted_mse: 0.0999\n",
      "Epoch 25/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.0983 - mae: 0.2264 - mse: 0.1931 - weighted_mae: 0.2202 - weighted_mse: 0.1823 - val_loss: 0.0335 - val_mae: 0.1052 - val_mse: 0.1108 - val_weighted_mae: 0.1021 - val_weighted_mse: 0.1081\n",
      "Epoch 26/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.1011 - mae: 0.2270 - mse: 0.1962 - weighted_mae: 0.2219 - weighted_mse: 0.1876 - val_loss: 0.0229 - val_mae: 0.1044 - val_mse: 0.0752 - val_weighted_mae: 0.1027 - val_weighted_mse: 0.0739\n",
      "Epoch 27/50\n",
      "2752/2752 [==============================] - 3s 995us/step - loss: 0.0987 - mae: 0.2250 - mse: 0.1916 - weighted_mae: 0.2198 - weighted_mse: 0.1830 - val_loss: 0.0234 - val_mae: 0.0995 - val_mse: 0.0775 - val_weighted_mae: 0.0979 - val_weighted_mse: 0.0755\n",
      "Epoch 28/50\n",
      "2752/2752 [==============================] - 3s 987us/step - loss: 0.1004 - mae: 0.2268 - mse: 0.1960 - weighted_mae: 0.2211 - weighted_mse: 0.1861 - val_loss: 0.0207 - val_mae: 0.0939 - val_mse: 0.0689 - val_weighted_mae: 0.0916 - val_weighted_mse: 0.0668\n",
      "Epoch 29/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.0993 - mae: 0.2250 - mse: 0.1927 - weighted_mae: 0.2195 - weighted_mse: 0.1841 - val_loss: 0.0229 - val_mae: 0.0935 - val_mse: 0.0751 - val_weighted_mae: 0.0916 - val_weighted_mse: 0.0737\n",
      "Epoch 30/50\n",
      "2752/2752 [==============================] - 3s 916us/step - loss: 0.1024 - mae: 0.2267 - mse: 0.1979 - weighted_mae: 0.2212 - weighted_mse: 0.1900 - val_loss: 0.0287 - val_mae: 0.1001 - val_mse: 0.0953 - val_weighted_mae: 0.0986 - val_weighted_mse: 0.0926\n",
      "Epoch 31/50\n",
      "2752/2752 [==============================] - 3s 915us/step - loss: 0.0991 - mae: 0.2249 - mse: 0.1929 - weighted_mae: 0.2194 - weighted_mse: 0.1838 - val_loss: 0.0280 - val_mae: 0.1100 - val_mse: 0.0913 - val_weighted_mae: 0.1081 - val_weighted_mse: 0.0905\n",
      "Epoch 32/50\n",
      "2752/2752 [==============================] - 2s 905us/step - loss: 0.0961 - mae: 0.2231 - mse: 0.1886 - weighted_mae: 0.2174 - weighted_mse: 0.1782 - val_loss: 0.0197 - val_mae: 0.0876 - val_mse: 0.0653 - val_weighted_mae: 0.0858 - val_weighted_mse: 0.0635\n",
      "Epoch 33/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.0959 - mae: 0.2229 - mse: 0.1884 - weighted_mae: 0.2171 - weighted_mse: 0.1779 - val_loss: 0.0252 - val_mae: 0.1214 - val_mse: 0.0822 - val_weighted_mae: 0.1197 - val_weighted_mse: 0.0812\n",
      "Epoch 34/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.1023 - mae: 0.2260 - mse: 0.1981 - weighted_mae: 0.2209 - weighted_mse: 0.1897 - val_loss: 0.0241 - val_mae: 0.0975 - val_mse: 0.0795 - val_weighted_mae: 0.0962 - val_weighted_mse: 0.0777\n",
      "Epoch 35/50\n",
      "2752/2752 [==============================] - 3s 1ms/step - loss: 0.0991 - mae: 0.2241 - mse: 0.1915 - weighted_mae: 0.2194 - weighted_mse: 0.1837 - val_loss: 0.0329 - val_mae: 0.1025 - val_mse: 0.1114 - val_weighted_mae: 0.1002 - val_weighted_mse: 0.1060\n",
      "Epoch 36/50\n",
      "2752/2752 [==============================] - 2s 896us/step - loss: 0.0996 - mae: 0.2237 - mse: 0.1926 - weighted_mae: 0.2182 - weighted_mse: 0.1847 - val_loss: 0.0208 - val_mae: 0.1155 - val_mse: 0.0685 - val_weighted_mae: 0.1150 - val_weighted_mse: 0.0672\n",
      "Epoch 37/50\n",
      "2752/2752 [==============================] - 3s 973us/step - loss: 0.0941 - mae: 0.2223 - mse: 0.1848 - weighted_mae: 0.2167 - weighted_mse: 0.1744 - val_loss: 0.0206 - val_mae: 0.1252 - val_mse: 0.0670 - val_weighted_mae: 0.1244 - val_weighted_mse: 0.0665\n",
      "Epoch 38/50\n",
      "2752/2752 [==============================] - 2s 886us/step - loss: 0.0970 - mae: 0.2223 - mse: 0.1899 - weighted_mae: 0.2168 - weighted_mse: 0.1799 - val_loss: 0.0330 - val_mae: 0.1049 - val_mse: 0.1095 - val_weighted_mae: 0.1029 - val_weighted_mse: 0.1064\n",
      "Epoch 39/50\n",
      "2752/2752 [==============================] - 3s 951us/step - loss: 0.0982 - mae: 0.2219 - mse: 0.1898 - weighted_mae: 0.2168 - weighted_mse: 0.1822 - val_loss: 0.0212 - val_mae: 0.1010 - val_mse: 0.0700 - val_weighted_mae: 0.0990 - val_weighted_mse: 0.0685\n",
      "Epoch 40/50\n",
      "2752/2752 [==============================] - 3s 976us/step - loss: 0.0953 - mae: 0.2213 - mse: 0.1867 - weighted_mae: 0.2158 - weighted_mse: 0.1768 - val_loss: 0.0256 - val_mae: 0.1022 - val_mse: 0.0844 - val_weighted_mae: 0.1013 - val_weighted_mse: 0.0825\n",
      "Epoch 41/50\n",
      "2752/2752 [==============================] - 3s 916us/step - loss: 0.0939 - mae: 0.2216 - mse: 0.1840 - weighted_mae: 0.2162 - weighted_mse: 0.1742 - val_loss: 0.0220 - val_mae: 0.0926 - val_mse: 0.0730 - val_weighted_mae: 0.0900 - val_weighted_mse: 0.0710\n",
      "Epoch 42/50\n",
      "2752/2752 [==============================] - 3s 927us/step - loss: 0.0936 - mae: 0.2195 - mse: 0.1824 - weighted_mae: 0.2143 - weighted_mse: 0.1736 - val_loss: 0.0214 - val_mae: 0.1086 - val_mse: 0.0702 - val_weighted_mae: 0.1075 - val_weighted_mse: 0.0689\n",
      "Training time: 2 minutes and 13.68 seconds\n",
      "MAE: 0.1194, R2: 0.9795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyle/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://c1a4d031-9fd3-4954-adcf-ec157a2a662b/assets\n",
      "Model saved as FPL_NeuralNet_20241017.joblib\n",
      "Scaler saved as FPL_NeuralNet_scaler.pkl\n",
      "\n",
      "Processing game week: 20242502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyle/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/kyle/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3459/3459 [==============================] - 3s 786us/step - loss: 0.0871 - mae: 0.2159 - mse: 0.1798 - weighted_mae: 0.2138 - weighted_mse: 0.1755 - val_loss: 0.1713 - val_mae: 0.1700 - val_mse: 0.1713 - val_weighted_mae: 0.1700 - val_weighted_mse: 0.1713\n",
      "Epoch 2/50\n",
      "3459/3459 [==============================] - 3s 760us/step - loss: 0.0876 - mae: 0.2148 - mse: 0.1796 - weighted_mae: 0.2128 - weighted_mse: 0.1765 - val_loss: 0.0938 - val_mae: 0.1612 - val_mse: 0.0938 - val_weighted_mae: 0.1612 - val_weighted_mse: 0.0938\n",
      "Epoch 3/50\n",
      "3459/3459 [==============================] - 3s 760us/step - loss: 0.0881 - mae: 0.2162 - mse: 0.1808 - weighted_mae: 0.2142 - weighted_mse: 0.1775 - val_loss: 0.1613 - val_mae: 0.1725 - val_mse: 0.1613 - val_weighted_mae: 0.1725 - val_weighted_mse: 0.1613\n",
      "Epoch 4/50\n",
      "3459/3459 [==============================] - 3s 771us/step - loss: 0.0837 - mae: 0.2116 - mse: 0.1718 - weighted_mae: 0.2098 - weighted_mse: 0.1687 - val_loss: 0.1380 - val_mae: 0.1568 - val_mse: 0.1380 - val_weighted_mae: 0.1568 - val_weighted_mse: 0.1380\n",
      "Epoch 5/50\n",
      "3459/3459 [==============================] - 3s 758us/step - loss: 0.0852 - mae: 0.2132 - mse: 0.1754 - weighted_mae: 0.2115 - weighted_mse: 0.1718 - val_loss: 0.1108 - val_mae: 0.1525 - val_mse: 0.1108 - val_weighted_mae: 0.1525 - val_weighted_mse: 0.1108\n",
      "Epoch 6/50\n",
      "3459/3459 [==============================] - 3s 759us/step - loss: 0.0845 - mae: 0.2120 - mse: 0.1717 - weighted_mae: 0.2105 - weighted_mse: 0.1704 - val_loss: 0.1790 - val_mae: 0.1802 - val_mse: 0.1790 - val_weighted_mae: 0.1802 - val_weighted_mse: 0.1790\n",
      "Epoch 7/50\n",
      "3459/3459 [==============================] - 3s 758us/step - loss: 0.0833 - mae: 0.2109 - mse: 0.1705 - weighted_mae: 0.2092 - weighted_mse: 0.1679 - val_loss: 0.1278 - val_mae: 0.1559 - val_mse: 0.1278 - val_weighted_mae: 0.1559 - val_weighted_mse: 0.1278\n",
      "Epoch 8/50\n",
      "3459/3459 [==============================] - 3s 758us/step - loss: 0.0853 - mae: 0.2118 - mse: 0.1735 - weighted_mae: 0.2103 - weighted_mse: 0.1718 - val_loss: 0.3039 - val_mae: 0.2114 - val_mse: 0.3039 - val_weighted_mae: 0.2114 - val_weighted_mse: 0.3039\n",
      "Epoch 9/50\n",
      "3459/3459 [==============================] - 3s 761us/step - loss: 0.0839 - mae: 0.2103 - mse: 0.1715 - weighted_mae: 0.2086 - weighted_mse: 0.1692 - val_loss: 0.0999 - val_mae: 0.1464 - val_mse: 0.0999 - val_weighted_mae: 0.1464 - val_weighted_mse: 0.0999\n",
      "Epoch 10/50\n",
      "3459/3459 [==============================] - 3s 770us/step - loss: 0.0827 - mae: 0.2098 - mse: 0.1715 - weighted_mae: 0.2080 - weighted_mse: 0.1667 - val_loss: 0.1151 - val_mae: 0.1541 - val_mse: 0.1151 - val_weighted_mae: 0.1541 - val_weighted_mse: 0.1151\n",
      "Epoch 11/50\n",
      "3459/3459 [==============================] - 3s 759us/step - loss: 0.0858 - mae: 0.2115 - mse: 0.1760 - weighted_mae: 0.2097 - weighted_mse: 0.1729 - val_loss: 0.1237 - val_mae: 0.1411 - val_mse: 0.1237 - val_weighted_mae: 0.1411 - val_weighted_mse: 0.1237\n",
      "Epoch 12/50\n",
      "3459/3459 [==============================] - 3s 759us/step - loss: 0.0825 - mae: 0.2088 - mse: 0.1688 - weighted_mae: 0.2072 - weighted_mse: 0.1664 - val_loss: 0.1293 - val_mae: 0.1732 - val_mse: 0.1293 - val_weighted_mae: 0.1732 - val_weighted_mse: 0.1293\n",
      "Training time: 0 minutes and 45.89 seconds\n",
      "MAE: 0.1612, R2: 0.9862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyle/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://df2ff220-2014-45ed-8f3b-75c05c407eae/assets\n",
      "Model saved as FPL_NeuralNet_20241017.joblib\n",
      "Scaler saved as FPL_NeuralNet_scaler.pkl\n",
      "\n",
      "Processing game week: 20242503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyle/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/kyle/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3479/3479 [==============================] - 3s 782us/step - loss: 0.0868 - mae: 0.2145 - mse: 0.1781 - weighted_mae: 0.2124 - weighted_mse: 0.1739 - val_loss: 0.1316 - val_mae: 0.1437 - val_mse: 0.1316 - val_weighted_mae: 0.1437 - val_weighted_mse: 0.1316\n",
      "Epoch 2/50\n",
      "3479/3479 [==============================] - 3s 772us/step - loss: 0.0894 - mae: 0.2172 - mse: 0.1837 - weighted_mae: 0.2148 - weighted_mse: 0.1791 - val_loss: 0.1287 - val_mae: 0.1441 - val_mse: 0.1287 - val_weighted_mae: 0.1441 - val_weighted_mse: 0.1287\n",
      "Epoch 3/50\n",
      "3479/3479 [==============================] - 3s 772us/step - loss: 0.0879 - mae: 0.2137 - mse: 0.1784 - weighted_mae: 0.2125 - weighted_mse: 0.1761 - val_loss: 0.1950 - val_mae: 0.1629 - val_mse: 0.1950 - val_weighted_mae: 0.1629 - val_weighted_mse: 0.1950\n",
      "Epoch 4/50\n",
      "3479/3479 [==============================] - 3s 768us/step - loss: 0.0845 - mae: 0.2125 - mse: 0.1741 - weighted_mae: 0.2103 - weighted_mse: 0.1694 - val_loss: 0.0724 - val_mae: 0.1070 - val_mse: 0.0724 - val_weighted_mae: 0.1070 - val_weighted_mse: 0.0724\n",
      "Epoch 5/50\n",
      "3479/3479 [==============================] - 3s 780us/step - loss: 0.0871 - mae: 0.2125 - mse: 0.1768 - weighted_mae: 0.2111 - weighted_mse: 0.1745 - val_loss: 0.1275 - val_mae: 0.1283 - val_mse: 0.1275 - val_weighted_mae: 0.1283 - val_weighted_mse: 0.1275\n",
      "Epoch 6/50\n",
      "3479/3479 [==============================] - 3s 767us/step - loss: 0.0886 - mae: 0.2137 - mse: 0.1790 - weighted_mae: 0.2127 - weighted_mse: 0.1775 - val_loss: 0.0941 - val_mae: 0.1115 - val_mse: 0.0941 - val_weighted_mae: 0.1115 - val_weighted_mse: 0.0941\n",
      "Epoch 7/50\n",
      "3479/3479 [==============================] - 3s 780us/step - loss: 0.0880 - mae: 0.2141 - mse: 0.1793 - weighted_mae: 0.2125 - weighted_mse: 0.1763 - val_loss: 0.0781 - val_mae: 0.1070 - val_mse: 0.0781 - val_weighted_mae: 0.1070 - val_weighted_mse: 0.0781\n",
      "Epoch 8/50\n",
      "3479/3479 [==============================] - 3s 773us/step - loss: 0.0854 - mae: 0.2116 - mse: 0.1736 - weighted_mae: 0.2101 - weighted_mse: 0.1711 - val_loss: 0.0750 - val_mae: 0.1133 - val_mse: 0.0750 - val_weighted_mae: 0.1133 - val_weighted_mse: 0.0750\n",
      "Epoch 9/50\n",
      "3479/3479 [==============================] - 3s 769us/step - loss: 0.0862 - mae: 0.2126 - mse: 0.1761 - weighted_mae: 0.2107 - weighted_mse: 0.1728 - val_loss: 0.0641 - val_mae: 0.1037 - val_mse: 0.0641 - val_weighted_mae: 0.1037 - val_weighted_mse: 0.0641\n",
      "Epoch 10/50\n",
      "3479/3479 [==============================] - 3s 768us/step - loss: 0.0850 - mae: 0.2109 - mse: 0.1720 - weighted_mae: 0.2096 - weighted_mse: 0.1704 - val_loss: 0.0840 - val_mae: 0.1220 - val_mse: 0.0840 - val_weighted_mae: 0.1220 - val_weighted_mse: 0.0840\n",
      "Epoch 11/50\n",
      "3479/3479 [==============================] - 3s 781us/step - loss: 0.0842 - mae: 0.2102 - mse: 0.1721 - weighted_mae: 0.2087 - weighted_mse: 0.1687 - val_loss: 0.1197 - val_mae: 0.1561 - val_mse: 0.1197 - val_weighted_mae: 0.1561 - val_weighted_mse: 0.1197\n",
      "Epoch 12/50\n",
      "3479/3479 [==============================] - 3s 774us/step - loss: 0.0858 - mae: 0.2119 - mse: 0.1747 - weighted_mae: 0.2102 - weighted_mse: 0.1718 - val_loss: 0.0706 - val_mae: 0.1038 - val_mse: 0.0706 - val_weighted_mae: 0.1038 - val_weighted_mse: 0.0706\n",
      "Epoch 13/50\n",
      "3479/3479 [==============================] - 3s 830us/step - loss: 0.0855 - mae: 0.2108 - mse: 0.1757 - weighted_mae: 0.2089 - weighted_mse: 0.1714 - val_loss: 0.1058 - val_mae: 0.1245 - val_mse: 0.1058 - val_weighted_mae: 0.1245 - val_weighted_mse: 0.1058\n",
      "Epoch 14/50\n",
      "3479/3479 [==============================] - 3s 957us/step - loss: 0.0829 - mae: 0.2089 - mse: 0.1695 - weighted_mae: 0.2070 - weighted_mse: 0.1661 - val_loss: 0.0611 - val_mae: 0.1018 - val_mse: 0.0611 - val_weighted_mae: 0.1018 - val_weighted_mse: 0.0611\n",
      "Epoch 15/50\n",
      "3479/3479 [==============================] - 4s 1ms/step - loss: 0.0822 - mae: 0.2088 - mse: 0.1669 - weighted_mae: 0.2074 - weighted_mse: 0.1647 - val_loss: 0.1304 - val_mae: 0.1293 - val_mse: 0.1304 - val_weighted_mae: 0.1293 - val_weighted_mse: 0.1304\n",
      "Epoch 16/50\n",
      "3479/3479 [==============================] - 4s 1ms/step - loss: 0.0826 - mae: 0.2089 - mse: 0.1684 - weighted_mae: 0.2076 - weighted_mse: 0.1655 - val_loss: 0.0826 - val_mae: 0.1094 - val_mse: 0.0826 - val_weighted_mae: 0.1094 - val_weighted_mse: 0.0826\n",
      "Epoch 17/50\n",
      "3479/3479 [==============================] - 4s 1ms/step - loss: 0.0839 - mae: 0.2094 - mse: 0.1698 - weighted_mae: 0.2080 - weighted_mse: 0.1682 - val_loss: 0.0696 - val_mae: 0.1025 - val_mse: 0.0696 - val_weighted_mae: 0.1025 - val_weighted_mse: 0.0696\n",
      "Epoch 18/50\n",
      "3479/3479 [==============================] - 3s 965us/step - loss: 0.0823 - mae: 0.2077 - mse: 0.1675 - weighted_mae: 0.2062 - weighted_mse: 0.1649 - val_loss: 0.0772 - val_mae: 0.1020 - val_mse: 0.0772 - val_weighted_mae: 0.1020 - val_weighted_mse: 0.0772\n",
      "Epoch 19/50\n",
      "3479/3479 [==============================] - 3s 950us/step - loss: 0.0818 - mae: 0.2074 - mse: 0.1664 - weighted_mae: 0.2061 - weighted_mse: 0.1640 - val_loss: 0.0766 - val_mae: 0.1179 - val_mse: 0.0766 - val_weighted_mae: 0.1179 - val_weighted_mse: 0.0766\n",
      "Epoch 20/50\n",
      "3479/3479 [==============================] - 3s 855us/step - loss: 0.0818 - mae: 0.2071 - mse: 0.1671 - weighted_mae: 0.2055 - weighted_mse: 0.1639 - val_loss: 0.0972 - val_mae: 0.1199 - val_mse: 0.0972 - val_weighted_mae: 0.1199 - val_weighted_mse: 0.0972\n",
      "Epoch 21/50\n",
      "3479/3479 [==============================] - 3s 865us/step - loss: 0.0842 - mae: 0.2082 - mse: 0.1715 - weighted_mae: 0.2070 - weighted_mse: 0.1688 - val_loss: 0.1102 - val_mae: 0.1242 - val_mse: 0.1102 - val_weighted_mae: 0.1242 - val_weighted_mse: 0.1102\n",
      "Epoch 22/50\n",
      "3479/3479 [==============================] - 3s 851us/step - loss: 0.0822 - mae: 0.2080 - mse: 0.1689 - weighted_mae: 0.2061 - weighted_mse: 0.1647 - val_loss: 0.0698 - val_mae: 0.1039 - val_mse: 0.0698 - val_weighted_mae: 0.1039 - val_weighted_mse: 0.0698\n",
      "Epoch 23/50\n",
      "3479/3479 [==============================] - 3s 903us/step - loss: 0.0828 - mae: 0.2085 - mse: 0.1697 - weighted_mae: 0.2067 - weighted_mse: 0.1659 - val_loss: 0.1242 - val_mae: 0.1245 - val_mse: 0.1242 - val_weighted_mae: 0.1245 - val_weighted_mse: 0.1242\n",
      "Epoch 24/50\n",
      "3479/3479 [==============================] - 3s 939us/step - loss: 0.0801 - mae: 0.2065 - mse: 0.1641 - weighted_mae: 0.2048 - weighted_mse: 0.1604 - val_loss: 0.0930 - val_mae: 0.1115 - val_mse: 0.0930 - val_weighted_mae: 0.1115 - val_weighted_mse: 0.0930\n",
      "Training time: 1 minutes and 25.32 seconds\n",
      "MAE: 0.1018, R2: 0.9870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyle/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://81e7aa02-c2c2-4c7f-a900-46f566de7542/assets\n",
      "Model saved as FPL_NeuralNet_20241017.joblib\n",
      "Scaler saved as FPL_NeuralNet_scaler.pkl\n",
      "\n",
      "Processing game week: 20242504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyle/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/kyle/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3499/3499 [==============================] - 3s 810us/step - loss: 0.0843 - mae: 0.2102 - mse: 0.1718 - weighted_mae: 0.2087 - weighted_mse: 0.1679 - val_loss: 0.1276 - val_mae: 0.1340 - val_mse: 0.1276 - val_weighted_mae: 0.1340 - val_weighted_mse: 0.1276\n",
      "Epoch 2/50\n",
      "3499/3499 [==============================] - 3s 819us/step - loss: 0.0835 - mae: 0.2088 - mse: 0.1700 - weighted_mae: 0.2069 - weighted_mse: 0.1664 - val_loss: 0.1396 - val_mae: 0.1394 - val_mse: 0.1396 - val_weighted_mae: 0.1394 - val_weighted_mse: 0.1396\n",
      "Epoch 3/50\n",
      "3499/3499 [==============================] - 3s 818us/step - loss: 0.0849 - mae: 0.2091 - mse: 0.1717 - weighted_mae: 0.2077 - weighted_mse: 0.1690 - val_loss: 0.1169 - val_mae: 0.1265 - val_mse: 0.1169 - val_weighted_mae: 0.1265 - val_weighted_mse: 0.1169\n",
      "Epoch 4/50\n",
      "3499/3499 [==============================] - 4s 1ms/step - loss: 0.0855 - mae: 0.2100 - mse: 0.1733 - weighted_mae: 0.2084 - weighted_mse: 0.1704 - val_loss: 0.2130 - val_mae: 0.1505 - val_mse: 0.2130 - val_weighted_mae: 0.1505 - val_weighted_mse: 0.2130\n",
      "Epoch 5/50\n",
      "3499/3499 [==============================] - 4s 1ms/step - loss: 0.0822 - mae: 0.2076 - mse: 0.1672 - weighted_mae: 0.2058 - weighted_mse: 0.1638 - val_loss: 0.1468 - val_mae: 0.1494 - val_mse: 0.1468 - val_weighted_mae: 0.1494 - val_weighted_mse: 0.1468\n",
      "Epoch 6/50\n",
      "3499/3499 [==============================] - 3s 949us/step - loss: 0.0837 - mae: 0.2085 - mse: 0.1689 - weighted_mae: 0.2074 - weighted_mse: 0.1667 - val_loss: 0.1212 - val_mae: 0.1358 - val_mse: 0.1212 - val_weighted_mae: 0.1358 - val_weighted_mse: 0.1212\n",
      "Epoch 7/50\n",
      "3499/3499 [==============================] - 3s 796us/step - loss: 0.0826 - mae: 0.2072 - mse: 0.1677 - weighted_mae: 0.2055 - weighted_mse: 0.1645 - val_loss: 0.1067 - val_mae: 0.1353 - val_mse: 0.1067 - val_weighted_mae: 0.1353 - val_weighted_mse: 0.1067\n",
      "Epoch 8/50\n",
      "3499/3499 [==============================] - 3s 790us/step - loss: 0.0849 - mae: 0.2090 - mse: 0.1704 - weighted_mae: 0.2078 - weighted_mse: 0.1692 - val_loss: 0.1569 - val_mae: 0.1345 - val_mse: 0.1569 - val_weighted_mae: 0.1345 - val_weighted_mse: 0.1569\n",
      "Epoch 9/50\n",
      "3499/3499 [==============================] - 3s 787us/step - loss: 0.0823 - mae: 0.2085 - mse: 0.1691 - weighted_mae: 0.2061 - weighted_mse: 0.1639 - val_loss: 0.1345 - val_mae: 0.1319 - val_mse: 0.1345 - val_weighted_mae: 0.1319 - val_weighted_mse: 0.1345\n",
      "Epoch 10/50\n",
      "3499/3499 [==============================] - 3s 792us/step - loss: 0.0810 - mae: 0.2068 - mse: 0.1650 - weighted_mae: 0.2050 - weighted_mse: 0.1614 - val_loss: 0.1060 - val_mae: 0.1098 - val_mse: 0.1060 - val_weighted_mae: 0.1098 - val_weighted_mse: 0.1060\n",
      "Epoch 11/50\n",
      "3499/3499 [==============================] - 3s 818us/step - loss: 0.0830 - mae: 0.2071 - mse: 0.1686 - weighted_mae: 0.2055 - weighted_mse: 0.1654 - val_loss: 0.1966 - val_mae: 0.1585 - val_mse: 0.1966 - val_weighted_mae: 0.1585 - val_weighted_mse: 0.1966\n",
      "Epoch 12/50\n",
      "3499/3499 [==============================] - 3s 820us/step - loss: 0.0812 - mae: 0.2062 - mse: 0.1648 - weighted_mae: 0.2048 - weighted_mse: 0.1617 - val_loss: 0.1214 - val_mae: 0.1324 - val_mse: 0.1214 - val_weighted_mae: 0.1324 - val_weighted_mse: 0.1214\n",
      "Epoch 13/50\n",
      "3499/3499 [==============================] - 3s 819us/step - loss: 0.0804 - mae: 0.2069 - mse: 0.1645 - weighted_mae: 0.2049 - weighted_mse: 0.1602 - val_loss: 0.0916 - val_mae: 0.1254 - val_mse: 0.0916 - val_weighted_mae: 0.1254 - val_weighted_mse: 0.0916\n",
      "Epoch 14/50\n",
      "3499/3499 [==============================] - 3s 786us/step - loss: 0.0821 - mae: 0.2065 - mse: 0.1657 - weighted_mae: 0.2051 - weighted_mse: 0.1635 - val_loss: 0.0949 - val_mae: 0.1160 - val_mse: 0.0949 - val_weighted_mae: 0.1160 - val_weighted_mse: 0.0949\n",
      "Epoch 15/50\n",
      "3499/3499 [==============================] - 3s 826us/step - loss: 0.0805 - mae: 0.2052 - mse: 0.1639 - weighted_mae: 0.2037 - weighted_mse: 0.1604 - val_loss: 0.0896 - val_mae: 0.1155 - val_mse: 0.0896 - val_weighted_mae: 0.1155 - val_weighted_mse: 0.0896\n",
      "Epoch 16/50\n",
      "3499/3499 [==============================] - 3s 815us/step - loss: 0.0815 - mae: 0.2063 - mse: 0.1659 - weighted_mae: 0.2046 - weighted_mse: 0.1624 - val_loss: 0.1398 - val_mae: 0.1291 - val_mse: 0.1398 - val_weighted_mae: 0.1291 - val_weighted_mse: 0.1398\n",
      "Epoch 17/50\n",
      "3499/3499 [==============================] - 3s 807us/step - loss: 0.0794 - mae: 0.2052 - mse: 0.1623 - weighted_mae: 0.2033 - weighted_mse: 0.1582 - val_loss: 0.0859 - val_mae: 0.1433 - val_mse: 0.0859 - val_weighted_mae: 0.1433 - val_weighted_mse: 0.0859\n",
      "Epoch 18/50\n",
      "3499/3499 [==============================] - 3s 818us/step - loss: 0.0799 - mae: 0.2053 - mse: 0.1629 - weighted_mae: 0.2034 - weighted_mse: 0.1591 - val_loss: 0.1273 - val_mae: 0.1118 - val_mse: 0.1273 - val_weighted_mae: 0.1118 - val_weighted_mse: 0.1273\n",
      "Epoch 19/50\n",
      "3499/3499 [==============================] - 3s 799us/step - loss: 0.0821 - mae: 0.2056 - mse: 0.1661 - weighted_mae: 0.2040 - weighted_mse: 0.1636 - val_loss: 0.0943 - val_mae: 0.1151 - val_mse: 0.0943 - val_weighted_mae: 0.1151 - val_weighted_mse: 0.0943\n",
      "Epoch 20/50\n",
      "3499/3499 [==============================] - 3s 792us/step - loss: 0.0797 - mae: 0.2046 - mse: 0.1606 - weighted_mae: 0.2033 - weighted_mse: 0.1588 - val_loss: 0.1248 - val_mae: 0.1259 - val_mse: 0.1248 - val_weighted_mae: 0.1259 - val_weighted_mse: 0.1248\n",
      "Epoch 21/50\n",
      "3499/3499 [==============================] - 3s 812us/step - loss: 0.0818 - mae: 0.2067 - mse: 0.1669 - weighted_mae: 0.2048 - weighted_mse: 0.1631 - val_loss: 0.0823 - val_mae: 0.1202 - val_mse: 0.0823 - val_weighted_mae: 0.1202 - val_weighted_mse: 0.0823\n",
      "Epoch 22/50\n",
      "3499/3499 [==============================] - 3s 802us/step - loss: 0.0793 - mae: 0.2049 - mse: 0.1609 - weighted_mae: 0.2034 - weighted_mse: 0.1580 - val_loss: 0.1509 - val_mae: 0.1358 - val_mse: 0.1509 - val_weighted_mae: 0.1358 - val_weighted_mse: 0.1509\n",
      "Epoch 23/50\n",
      "3499/3499 [==============================] - 3s 813us/step - loss: 0.0804 - mae: 0.2049 - mse: 0.1639 - weighted_mae: 0.2032 - weighted_mse: 0.1602 - val_loss: 0.2911 - val_mae: 0.1723 - val_mse: 0.2911 - val_weighted_mae: 0.1723 - val_weighted_mse: 0.2911\n",
      "Epoch 24/50\n",
      "3499/3499 [==============================] - 3s 811us/step - loss: 0.0841 - mae: 0.2079 - mse: 0.1710 - weighted_mae: 0.2060 - weighted_mse: 0.1676 - val_loss: 0.0945 - val_mae: 0.1407 - val_mse: 0.0945 - val_weighted_mae: 0.1407 - val_weighted_mse: 0.0945\n",
      "Epoch 25/50\n",
      "3499/3499 [==============================] - 3s 811us/step - loss: 0.0806 - mae: 0.2055 - mse: 0.1648 - weighted_mae: 0.2035 - weighted_mse: 0.1606 - val_loss: 0.1655 - val_mae: 0.1268 - val_mse: 0.1655 - val_weighted_mae: 0.1268 - val_weighted_mse: 0.1655\n",
      "Epoch 26/50\n",
      "3499/3499 [==============================] - 3s 774us/step - loss: 0.0823 - mae: 0.2068 - mse: 0.1670 - weighted_mae: 0.2046 - weighted_mse: 0.1640 - val_loss: 0.1390 - val_mae: 0.1191 - val_mse: 0.1390 - val_weighted_mae: 0.1191 - val_weighted_mse: 0.1390\n",
      "Epoch 27/50\n",
      "3499/3499 [==============================] - 3s 765us/step - loss: 0.0801 - mae: 0.2052 - mse: 0.1626 - weighted_mae: 0.2035 - weighted_mse: 0.1595 - val_loss: 0.1454 - val_mae: 0.1442 - val_mse: 0.1454 - val_weighted_mae: 0.1442 - val_weighted_mse: 0.1454\n",
      "Epoch 28/50\n",
      "3499/3499 [==============================] - 3s 766us/step - loss: 0.0810 - mae: 0.2052 - mse: 0.1630 - weighted_mae: 0.2038 - weighted_mse: 0.1614 - val_loss: 0.2312 - val_mae: 0.1488 - val_mse: 0.2312 - val_weighted_mae: 0.1488 - val_weighted_mse: 0.2312\n",
      "Epoch 29/50\n",
      "3499/3499 [==============================] - 3s 765us/step - loss: 0.0813 - mae: 0.2058 - mse: 0.1647 - weighted_mae: 0.2043 - weighted_mse: 0.1619 - val_loss: 0.1062 - val_mae: 0.1415 - val_mse: 0.1062 - val_weighted_mae: 0.1415 - val_weighted_mse: 0.1062\n",
      "Epoch 30/50\n",
      "3499/3499 [==============================] - 3s 786us/step - loss: 0.0803 - mae: 0.2051 - mse: 0.1635 - weighted_mae: 0.2033 - weighted_mse: 0.1600 - val_loss: 0.1202 - val_mae: 0.1336 - val_mse: 0.1202 - val_weighted_mae: 0.1336 - val_weighted_mse: 0.1202\n",
      "Epoch 31/50\n",
      "3499/3499 [==============================] - 3s 778us/step - loss: 0.0790 - mae: 0.2039 - mse: 0.1605 - weighted_mae: 0.2022 - weighted_mse: 0.1573 - val_loss: 0.0830 - val_mae: 0.1145 - val_mse: 0.0830 - val_weighted_mae: 0.1145 - val_weighted_mse: 0.0830\n",
      "Training time: 1 minutes and 43.90 seconds\n",
      "MAE: 0.1202, R2: 0.9849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyle/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://fe53f07a-405c-4dac-84c7-8e3ec0b140d3/assets\n",
      "Model saved as FPL_NeuralNet_20241017.joblib\n",
      "Scaler saved as FPL_NeuralNet_scaler.pkl\n",
      "\n",
      "Processing game week: 20242505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyle/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/kyle/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3520/3520 [==============================] - 4s 1ms/step - loss: 0.0806 - mae: 0.2051 - mse: 0.1621 - weighted_mae: 0.2035 - weighted_mse: 0.1597 - val_loss: 0.1295 - val_mae: 0.1434 - val_mse: 0.1295 - val_weighted_mae: 0.1434 - val_weighted_mse: 0.1295\n",
      "Epoch 2/50\n",
      "3520/3520 [==============================] - 4s 1ms/step - loss: 0.0802 - mae: 0.2043 - mse: 0.1619 - weighted_mae: 0.2025 - weighted_mse: 0.1589 - val_loss: 0.1202 - val_mae: 0.1387 - val_mse: 0.1202 - val_weighted_mae: 0.1387 - val_weighted_mse: 0.1202\n",
      "Epoch 3/50\n",
      "3520/3520 [==============================] - 3s 970us/step - loss: 0.0830 - mae: 0.2070 - mse: 0.1691 - weighted_mae: 0.2050 - weighted_mse: 0.1645 - val_loss: 0.1310 - val_mae: 0.1427 - val_mse: 0.1310 - val_weighted_mae: 0.1427 - val_weighted_mse: 0.1310\n",
      "Epoch 4/50\n",
      "3520/3520 [==============================] - 3s 915us/step - loss: 0.0803 - mae: 0.2051 - mse: 0.1610 - weighted_mae: 0.2040 - weighted_mse: 0.1590 - val_loss: 0.1123 - val_mae: 0.1218 - val_mse: 0.1123 - val_weighted_mae: 0.1218 - val_weighted_mse: 0.1123\n",
      "Epoch 5/50\n",
      "3520/3520 [==============================] - 3s 885us/step - loss: 0.0846 - mae: 0.2068 - mse: 0.1692 - weighted_mae: 0.2058 - weighted_mse: 0.1677 - val_loss: 0.1268 - val_mae: 0.1503 - val_mse: 0.1268 - val_weighted_mae: 0.1503 - val_weighted_mse: 0.1268\n",
      "Epoch 6/50\n",
      "3520/3520 [==============================] - 3s 910us/step - loss: 0.0805 - mae: 0.2051 - mse: 0.1633 - weighted_mae: 0.2032 - weighted_mse: 0.1595 - val_loss: 0.1080 - val_mae: 0.1245 - val_mse: 0.1080 - val_weighted_mae: 0.1245 - val_weighted_mse: 0.1080\n",
      "Epoch 7/50\n",
      "3520/3520 [==============================] - 3s 929us/step - loss: 0.0801 - mae: 0.2043 - mse: 0.1624 - weighted_mae: 0.2022 - weighted_mse: 0.1587 - val_loss: 0.1826 - val_mae: 0.1472 - val_mse: 0.1826 - val_weighted_mae: 0.1472 - val_weighted_mse: 0.1826\n",
      "Epoch 8/50\n",
      "3520/3520 [==============================] - 3s 880us/step - loss: 0.0777 - mae: 0.2015 - mse: 0.1566 - weighted_mae: 0.1999 - weighted_mse: 0.1539 - val_loss: 0.1523 - val_mae: 0.1379 - val_mse: 0.1523 - val_weighted_mae: 0.1379 - val_weighted_mse: 0.1523\n",
      "Epoch 9/50\n",
      "3520/3520 [==============================] - 3s 934us/step - loss: 0.0834 - mae: 0.2053 - mse: 0.1674 - weighted_mae: 0.2038 - weighted_mse: 0.1652 - val_loss: 0.1435 - val_mae: 0.1288 - val_mse: 0.1435 - val_weighted_mae: 0.1288 - val_weighted_mse: 0.1435\n",
      "Epoch 10/50\n",
      "3520/3520 [==============================] - 3s 894us/step - loss: 0.0816 - mae: 0.2046 - mse: 0.1634 - weighted_mae: 0.2032 - weighted_mse: 0.1617 - val_loss: 0.1255 - val_mae: 0.1344 - val_mse: 0.1255 - val_weighted_mae: 0.1344 - val_weighted_mse: 0.1255\n",
      "Epoch 11/50\n",
      "3520/3520 [==============================] - 3s 895us/step - loss: 0.0807 - mae: 0.2040 - mse: 0.1617 - weighted_mae: 0.2030 - weighted_mse: 0.1599 - val_loss: 0.1648 - val_mae: 0.1665 - val_mse: 0.1648 - val_weighted_mae: 0.1665 - val_weighted_mse: 0.1648\n",
      "Epoch 12/50\n",
      "3520/3520 [==============================] - 3s 907us/step - loss: 0.0793 - mae: 0.2039 - mse: 0.1606 - weighted_mae: 0.2022 - weighted_mse: 0.1570 - val_loss: 0.0999 - val_mae: 0.1149 - val_mse: 0.0999 - val_weighted_mae: 0.1149 - val_weighted_mse: 0.0999\n",
      "Epoch 13/50\n",
      "3520/3520 [==============================] - 3s 893us/step - loss: 0.0799 - mae: 0.2039 - mse: 0.1619 - weighted_mae: 0.2019 - weighted_mse: 0.1582 - val_loss: 0.1531 - val_mae: 0.1515 - val_mse: 0.1531 - val_weighted_mae: 0.1515 - val_weighted_mse: 0.1531\n",
      "Epoch 14/50\n",
      "3520/3520 [==============================] - 3s 937us/step - loss: 0.0800 - mae: 0.2034 - mse: 0.1621 - weighted_mae: 0.2012 - weighted_mse: 0.1585 - val_loss: 0.1156 - val_mae: 0.1504 - val_mse: 0.1156 - val_weighted_mae: 0.1504 - val_weighted_mse: 0.1156\n",
      "Epoch 15/50\n",
      "3520/3520 [==============================] - 3s 966us/step - loss: 0.0801 - mae: 0.2040 - mse: 0.1608 - weighted_mae: 0.2025 - weighted_mse: 0.1586 - val_loss: 0.1036 - val_mae: 0.1299 - val_mse: 0.1036 - val_weighted_mae: 0.1299 - val_weighted_mse: 0.1036\n",
      "Epoch 16/50\n",
      "3520/3520 [==============================] - 4s 1ms/step - loss: 0.0810 - mae: 0.2034 - mse: 0.1631 - weighted_mae: 0.2016 - weighted_mse: 0.1604 - val_loss: 0.1160 - val_mae: 0.1365 - val_mse: 0.1160 - val_weighted_mae: 0.1365 - val_weighted_mse: 0.1160\n",
      "Epoch 17/50\n",
      "3520/3520 [==============================] - 3s 911us/step - loss: 0.0809 - mae: 0.2034 - mse: 0.1633 - weighted_mae: 0.2016 - weighted_mse: 0.1602 - val_loss: 0.1170 - val_mae: 0.1293 - val_mse: 0.1170 - val_weighted_mae: 0.1293 - val_weighted_mse: 0.1170\n",
      "Epoch 18/50\n",
      "3520/3520 [==============================] - 3s 874us/step - loss: 0.0776 - mae: 0.2024 - mse: 0.1572 - weighted_mae: 0.2006 - weighted_mse: 0.1537 - val_loss: 0.1154 - val_mae: 0.1293 - val_mse: 0.1154 - val_weighted_mae: 0.1293 - val_weighted_mse: 0.1154\n",
      "Epoch 19/50\n",
      "3520/3520 [==============================] - 3s 906us/step - loss: 0.0801 - mae: 0.2040 - mse: 0.1624 - weighted_mae: 0.2017 - weighted_mse: 0.1588 - val_loss: 0.1362 - val_mae: 0.1316 - val_mse: 0.1362 - val_weighted_mae: 0.1316 - val_weighted_mse: 0.1362\n",
      "Epoch 20/50\n",
      "3520/3520 [==============================] - 3s 850us/step - loss: 0.0808 - mae: 0.2033 - mse: 0.1618 - weighted_mae: 0.2023 - weighted_mse: 0.1600 - val_loss: 0.1140 - val_mae: 0.1360 - val_mse: 0.1140 - val_weighted_mae: 0.1360 - val_weighted_mse: 0.1140\n",
      "Epoch 21/50\n",
      "3520/3520 [==============================] - 3s 893us/step - loss: 0.0807 - mae: 0.2034 - mse: 0.1625 - weighted_mae: 0.2015 - weighted_mse: 0.1599 - val_loss: 0.1372 - val_mae: 0.1650 - val_mse: 0.1372 - val_weighted_mae: 0.1650 - val_weighted_mse: 0.1372\n",
      "Epoch 22/50\n",
      "3520/3520 [==============================] - 3s 861us/step - loss: 0.0808 - mae: 0.2040 - mse: 0.1629 - weighted_mae: 0.2022 - weighted_mse: 0.1599 - val_loss: 0.1511 - val_mae: 0.1427 - val_mse: 0.1511 - val_weighted_mae: 0.1427 - val_weighted_mse: 0.1511\n",
      "Training time: 1 minutes and 26.95 seconds\n",
      "MAE: 0.1149, R2: 0.9815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyle/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://8c0e716c-cad4-42f2-9651-2eba998ae2ed/assets\n",
      "Model saved as FPL_NeuralNet_20241017.joblib\n",
      "Scaler saved as FPL_NeuralNet_scaler.pkl\n",
      "\n",
      "Processing game week: 20242506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyle/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/kyle/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3540/3540 [==============================] - 3s 808us/step - loss: 0.0819 - mae: 0.2047 - mse: 0.1636 - weighted_mae: 0.2033 - weighted_mse: 0.1613 - val_loss: 0.1348 - val_mae: 0.1381 - val_mse: 0.1348 - val_weighted_mae: 0.1381 - val_weighted_mse: 0.1348\n",
      "Epoch 2/50\n",
      "3540/3540 [==============================] - 3s 809us/step - loss: 0.0807 - mae: 0.2044 - mse: 0.1632 - weighted_mae: 0.2023 - weighted_mse: 0.1590 - val_loss: 0.1300 - val_mae: 0.1524 - val_mse: 0.1300 - val_weighted_mae: 0.1524 - val_weighted_mse: 0.1300\n",
      "Epoch 3/50\n",
      "3540/3540 [==============================] - 3s 843us/step - loss: 0.0807 - mae: 0.2030 - mse: 0.1605 - weighted_mae: 0.2015 - weighted_mse: 0.1590 - val_loss: 0.0867 - val_mae: 0.1149 - val_mse: 0.0867 - val_weighted_mae: 0.1149 - val_weighted_mse: 0.0867\n",
      "Epoch 4/50\n",
      "3540/3540 [==============================] - 3s 784us/step - loss: 0.0818 - mae: 0.2041 - mse: 0.1634 - weighted_mae: 0.2023 - weighted_mse: 0.1610 - val_loss: 0.0966 - val_mae: 0.1065 - val_mse: 0.0966 - val_weighted_mae: 0.1065 - val_weighted_mse: 0.0966\n",
      "Epoch 5/50\n",
      "3540/3540 [==============================] - 3s 847us/step - loss: 0.0810 - mae: 0.2040 - mse: 0.1640 - weighted_mae: 0.2020 - weighted_mse: 0.1596 - val_loss: 0.1820 - val_mae: 0.1593 - val_mse: 0.1820 - val_weighted_mae: 0.1593 - val_weighted_mse: 0.1820\n",
      "Epoch 6/50\n",
      "3540/3540 [==============================] - 3s 811us/step - loss: 0.0799 - mae: 0.2037 - mse: 0.1617 - weighted_mae: 0.2018 - weighted_mse: 0.1574 - val_loss: 0.1534 - val_mae: 0.1387 - val_mse: 0.1534 - val_weighted_mae: 0.1387 - val_weighted_mse: 0.1534\n",
      "Epoch 7/50\n",
      "3540/3540 [==============================] - 3s 809us/step - loss: 0.0818 - mae: 0.2043 - mse: 0.1638 - weighted_mae: 0.2028 - weighted_mse: 0.1611 - val_loss: 0.1509 - val_mae: 0.1470 - val_mse: 0.1509 - val_weighted_mae: 0.1470 - val_weighted_mse: 0.1509\n",
      "Epoch 8/50\n",
      "3540/3540 [==============================] - 3s 822us/step - loss: 0.0806 - mae: 0.2034 - mse: 0.1624 - weighted_mae: 0.2012 - weighted_mse: 0.1587 - val_loss: 0.1037 - val_mae: 0.1182 - val_mse: 0.1037 - val_weighted_mae: 0.1182 - val_weighted_mse: 0.1037\n",
      "Epoch 9/50\n",
      "3540/3540 [==============================] - 3s 817us/step - loss: 0.0825 - mae: 0.2047 - mse: 0.1648 - weighted_mae: 0.2031 - weighted_mse: 0.1625 - val_loss: 0.1118 - val_mae: 0.1396 - val_mse: 0.1118 - val_weighted_mae: 0.1396 - val_weighted_mse: 0.1118\n",
      "Epoch 10/50\n",
      "3540/3540 [==============================] - 3s 878us/step - loss: 0.0794 - mae: 0.2030 - mse: 0.1593 - weighted_mae: 0.2013 - weighted_mse: 0.1564 - val_loss: 0.2181 - val_mae: 0.1781 - val_mse: 0.2181 - val_weighted_mae: 0.1781 - val_weighted_mse: 0.2181\n",
      "Epoch 11/50\n",
      "3540/3540 [==============================] - 4s 1ms/step - loss: 0.0823 - mae: 0.2048 - mse: 0.1643 - weighted_mae: 0.2037 - weighted_mse: 0.1621 - val_loss: 0.1519 - val_mae: 0.1270 - val_mse: 0.1519 - val_weighted_mae: 0.1270 - val_weighted_mse: 0.1519\n",
      "Epoch 12/50\n",
      "3540/3540 [==============================] - 4s 995us/step - loss: 0.0810 - mae: 0.2036 - mse: 0.1620 - weighted_mae: 0.2023 - weighted_mse: 0.1596 - val_loss: 0.1138 - val_mae: 0.1201 - val_mse: 0.1138 - val_weighted_mae: 0.1201 - val_weighted_mse: 0.1138\n",
      "Epoch 13/50\n",
      "3540/3540 [==============================] - 3s 924us/step - loss: 0.0797 - mae: 0.2034 - mse: 0.1598 - weighted_mae: 0.2019 - weighted_mse: 0.1570 - val_loss: 0.0978 - val_mae: 0.1505 - val_mse: 0.0978 - val_weighted_mae: 0.1505 - val_weighted_mse: 0.0978\n",
      "Training time: 0 minutes and 54.24 seconds\n",
      "MAE: 0.1149, R2: 0.9851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyle/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://f32d9343-e0c3-4532-98c3-962d00f0fd02/assets\n",
      "Model saved as FPL_NeuralNet_20241017.joblib\n",
      "Scaler saved as FPL_NeuralNet_scaler.pkl\n",
      "\n",
      "Processing game week: 20242507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyle/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/Users/kyle/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3561/3561 [==============================] - 4s 1ms/step - loss: 0.0815 - mae: 0.2042 - mse: 0.1611 - weighted_mae: 0.2029 - weighted_mse: 0.1595 - val_loss: 0.1401 - val_mae: 0.1289 - val_mse: 0.1401 - val_weighted_mae: 0.1289 - val_weighted_mse: 0.1401\n",
      "Epoch 2/50\n",
      "3561/3561 [==============================] - 3s 973us/step - loss: 0.0833 - mae: 0.2046 - mse: 0.1642 - weighted_mae: 0.2035 - weighted_mse: 0.1631 - val_loss: 0.1877 - val_mae: 0.1471 - val_mse: 0.1877 - val_weighted_mae: 0.1471 - val_weighted_mse: 0.1877\n",
      "Epoch 3/50\n",
      "3561/3561 [==============================] - 3s 871us/step - loss: 0.0802 - mae: 0.2036 - mse: 0.1619 - weighted_mae: 0.2016 - weighted_mse: 0.1571 - val_loss: 0.1955 - val_mae: 0.1788 - val_mse: 0.1955 - val_weighted_mae: 0.1788 - val_weighted_mse: 0.1955\n",
      "Epoch 4/50\n",
      "3561/3561 [==============================] - 3s 851us/step - loss: 0.0829 - mae: 0.2043 - mse: 0.1636 - weighted_mae: 0.2030 - weighted_mse: 0.1623 - val_loss: 0.1630 - val_mae: 0.1291 - val_mse: 0.1630 - val_weighted_mae: 0.1291 - val_weighted_mse: 0.1630\n",
      "Epoch 5/50\n",
      "3561/3561 [==============================] - 3s 843us/step - loss: 0.0836 - mae: 0.2047 - mse: 0.1661 - weighted_mae: 0.2029 - weighted_mse: 0.1637 - val_loss: 0.1485 - val_mae: 0.1428 - val_mse: 0.1485 - val_weighted_mae: 0.1428 - val_weighted_mse: 0.1485\n",
      "Epoch 6/50\n",
      "3561/3561 [==============================] - 3s 874us/step - loss: 0.0829 - mae: 0.2044 - mse: 0.1630 - weighted_mae: 0.2035 - weighted_mse: 0.1623 - val_loss: 0.2465 - val_mae: 0.1556 - val_mse: 0.2465 - val_weighted_mae: 0.1556 - val_weighted_mse: 0.2465\n",
      "Epoch 7/50\n",
      "3561/3561 [==============================] - 3s 884us/step - loss: 0.0812 - mae: 0.2030 - mse: 0.1610 - weighted_mae: 0.2020 - weighted_mse: 0.1590 - val_loss: 0.1692 - val_mae: 0.1396 - val_mse: 0.1692 - val_weighted_mae: 0.1396 - val_weighted_mse: 0.1692\n",
      "Epoch 8/50\n",
      "3561/3561 [==============================] - 3s 889us/step - loss: 0.0790 - mae: 0.2021 - mse: 0.1587 - weighted_mae: 0.2004 - weighted_mse: 0.1548 - val_loss: 0.2039 - val_mae: 0.1428 - val_mse: 0.2039 - val_weighted_mae: 0.1428 - val_weighted_mse: 0.2039\n",
      "Epoch 9/50\n",
      "3561/3561 [==============================] - 3s 879us/step - loss: 0.0798 - mae: 0.2026 - mse: 0.1586 - weighted_mae: 0.2013 - weighted_mse: 0.1563 - val_loss: 0.1661 - val_mae: 0.1357 - val_mse: 0.1661 - val_weighted_mae: 0.1357 - val_weighted_mse: 0.1661\n",
      "Epoch 10/50\n",
      "3561/3561 [==============================] - 3s 871us/step - loss: 0.0795 - mae: 0.2015 - mse: 0.1587 - weighted_mae: 0.2001 - weighted_mse: 0.1557 - val_loss: 0.2028 - val_mae: 0.1724 - val_mse: 0.2028 - val_weighted_mae: 0.1724 - val_weighted_mse: 0.2028\n",
      "Epoch 11/50\n",
      "3561/3561 [==============================] - 3s 861us/step - loss: 0.0821 - mae: 0.2040 - mse: 0.1641 - weighted_mae: 0.2021 - weighted_mse: 0.1608 - val_loss: 0.2273 - val_mae: 0.1543 - val_mse: 0.2273 - val_weighted_mae: 0.1543 - val_weighted_mse: 0.2273\n",
      "Training time: 0 minutes and 51.26 seconds\n",
      "MAE: 0.1289, R2: 0.9772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyle/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://66ea1d7b-bfec-445c-a9f2-17d2406791dc/assets\n",
      "Model saved as FPL_NeuralNet_20241017.joblib\n",
      "Scaler saved as FPL_NeuralNet_scaler.pkl\n",
      "\n",
      "Overall MAE: 0.1231\n",
      "Overall R2: 0.9831\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "\n",
    "#Start time\n",
    "print(\"Start time:\", datetime.now().strftime(\"%m-%d-%Y %H:%M:%S\"))\n",
    "#------\n",
    "\n",
    "features = [\n",
    "    'position', 'xP', 'assists', 'clean_sheets', 'creativity', 'expected_assists',\n",
    "    'expected_goal_involvements', 'expected_goals', 'expected_goals_conceded', 'goals_conceded', \n",
    "    'goals_scored', 'influence', 'minutes', 'own_goals', 'penalties_missed', 'penalties_saved', \n",
    "    'red_cards', 'saves', 'starts', 'threat', 'transfers_balance', 'cost', 'was_home',\n",
    "    'yellow_cards', 'PlayerUniqueID', 'TeamUniqueID', 'TeamUniqueID_oppo', 'cumulative_points',\n",
    "    'cumulative_minutes', 'ppm', 'rolling_avg_points', 'rolling_avg_goals_scored', \n",
    "    'rolling_avg_goals_conceded', 'rolling_team_difficulty', 'game'\n",
    "]\n",
    "target = 'total_points'\n",
    "current_season_games = ['20242501', '20242502', '20242503', '20242504', '20242505', '20242506', '20242507']\n",
    "\n",
    "predictions, overall_mae, overall_r2, final_model = dynamic_model_training_with_updates(\n",
    "    player_history, \n",
    "    features, \n",
    "    target, \n",
    "    current_season_games,\n",
    "    MODEL_NAME,\n",
    "    SCALER_FILENAME\n",
    ")\n",
    "\n",
    "# To load the model later\n",
    "#loaded_model = load_model(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1662ad66",
   "metadata": {},
   "source": [
    "# Load Current Gameweek Data and run model\n",
    "\n",
    "\n",
    "1. **`load_and_preprocess_data()`**: \n",
    "   - Loads raw player data from a CSV file.\n",
    "   - Selects and renames columns, adds game week information, and calculates metrics such as points per minute (`ppm`) and average minutes played per game.\n",
    "   - Outputs the processed player data for the upcoming game week.\n",
    "\n",
    "2. **`load_and_process_fixtures()`**: \n",
    "   - Loads fixture data for the current season.\n",
    "   - Filters the data to get only the fixtures for the current game week.\n",
    "\n",
    "3. **`prepare_for_prediction(df, teams, players_unique, required_columns)`**:\n",
    "   - Merges player data with team data, cleaning up columns.\n",
    "   - Merges player data with unique player identifiers.\n",
    "   - Calculates rolling averages for performance metrics and team difficulty ratings.\n",
    "   - Outputs the prepared data for prediction.\n",
    "\n",
    "4. **`merge_player_and_fixture_data(next_gw, fixtures)`**:\n",
    "   - Merges player data with fixture data for both home and away teams.\n",
    "   - Calculates additional metrics like transfers balance, player cost, and whether the player is playing at home.\n",
    "   - Outputs the combined player and fixture data.\n",
    "\n",
    "5. **`calculate_rolling_averages(df, window=5)`**:\n",
    "   - Calculates rolling averages (over 5 games by default) for team-level statistics like goals scored and conceded.\n",
    "\n",
    "6. **`calculate_team_difficulty(df)`**:\n",
    "   - Calculates the average difficulty for each team based on fixture difficulty ratings and maps this back to the main dataframe.\n",
    "\n",
    "7. **`convert_dtypes_for_model(df, model_type)`**:\n",
    "   - Converts specific columns (like player and team IDs) into categorical data types, which are needed for model input.\n",
    "\n",
    "8. **`calculate_advanced_metrics(df)`**:\n",
    "   - Calculates advanced metrics like expected goals and a probability of scoring based on expected goals per minute.\n",
    "\n",
    "### Usage Flow:\n",
    "1. The script loads and preprocesses raw player data using `load_and_preprocess_data()`.\n",
    "2. Fixture data for the current game week is loaded with `load_and_process_fixtures()`.\n",
    "3. Player data is merged with fixture data using `merge_player_and_fixture_data()`.\n",
    "4. The merged data is prepared for model input with `prepare_for_prediction()`, which calculates rolling averages and team difficulty.\n",
    "5. Advanced metrics are computed with `calculate_advanced_metrics()`.\n",
    "6. The processed data is then scaled and passed to a trained neural network model, which predicts player points for the upcoming game week.\n",
    "\n",
    "Finally, the predicted points are normalized and added to the player data for analysis.\n",
    "\n",
    "\n",
    "This section is structured to prepare and analyze fantasy football data, facilitating predictions based on player and team performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9164a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    # Load raw player data\n",
    "    raw_2024_25 = pd.read_csv(RAW_URL)\n",
    "    \n",
    "    # Select and rename columns\n",
    "    columns_to_select = [\n",
    "        'element_type', 'ep_next', 'assists', 'bonus', 'bps', 'clean_sheets', 'creativity',\n",
    "        'expected_assists', 'expected_goal_involvements', 'expected_goals', 'expected_goals_conceded',\n",
    "        'goals_conceded', 'goals_scored', 'influence', 'minutes', 'own_goals', 'penalties_missed',\n",
    "        'penalties_saved', 'red_cards', 'saves', 'selected_by_percent', 'starts', 'threat',\n",
    "        'transfers_in', 'transfers_out', 'now_cost', 'yellow_cards', 'total_points',\n",
    "        'points_per_game', 'team', 'team_code', 'first_name', 'second_name'\n",
    "    ]\n",
    "    columns_to_rename = {\n",
    "        'element_type': 'position', 'ep_next': 'xP', 'now_cost': 'value',\n",
    "        'minutes': 'cumulative_minutes', 'total_points': 'cumulative_points',\n",
    "        'points_per_game': 'rolling_avg_points'\n",
    "    }\n",
    "    next_gw = raw_2024_25[columns_to_select].rename(columns=columns_to_rename)\n",
    "    \n",
    "    # Add game week info and calculate new columns\n",
    "    next_gw['game'] = GAME_ID\n",
    "    next_gw['ppm'] = next_gw['cumulative_points'] / next_gw['cumulative_minutes'].replace(0, 1)\n",
    "    next_gw['minutes'] = next_gw['cumulative_minutes'] / (CURRENT_GW - 1)\n",
    "    return next_gw\n",
    "\n",
    "def load_and_process_fixtures():\n",
    "    fixtures_2024_25 = pd.read_csv(FIXTURES_URL, usecols=['event', 'team_a', 'team_h', 'team_h_difficulty', 'team_a_difficulty'])\n",
    "    return fixtures_2024_25[fixtures_2024_25['event'] == CURRENT_GW]\n",
    "\n",
    "def prepare_for_prediction(df, teams, players_unique, required_columns):\n",
    "    # Merge with team data\n",
    "    teams_new = teams[teams['season'] == CURRENT_SEASON]\n",
    "    df = df.merge(teams_new, left_on='team', right_on='id', how='left')\n",
    "    df = df.merge(teams_new, left_on='team_h', right_on='id', how='left', suffixes=('', '_oppo'))\n",
    "    \n",
    "    # Clean up columns\n",
    "    columns_to_drop = ['id', 'id_oppo', 'team_name', 'team_name_oppo', 'season', 'season_oppo']\n",
    "    df.drop(columns=columns_to_drop, errors='ignore', inplace=True)\n",
    "    df.rename(columns={'TeamUniqueID': 'TeamUniqueID', 'TeamUniqueID_oppo': 'TeamUniqueID_oppo'}, inplace=True)\n",
    "    \n",
    "    # Merge with player data\n",
    "    df = df.merge(players_unique, on=['first_name', 'second_name'], how='left')\n",
    "    \n",
    "    # Calculate rolling averages\n",
    "    df = calculate_rolling_averages(df)\n",
    "    \n",
    "    # Calculate team difficulty\n",
    "    df = calculate_team_difficulty(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def merge_player_and_fixture_data(next_gw, fixtures):\n",
    "    # Merge for both home and away teams\n",
    "    merged_home = pd.merge(next_gw, fixtures, left_on='team', right_on='team_h', how='inner')\n",
    "    merged_away = pd.merge(next_gw, fixtures, left_on='team', right_on='team_a', how='inner')\n",
    "    final_merged_df = pd.concat([merged_home, merged_away])\n",
    "    \n",
    "    # Additional processing\n",
    "    final_merged_df['transfers_balance'] = final_merged_df['transfers_in'] - final_merged_df['transfers_out']\n",
    "    final_merged_df['cost'] = final_merged_df['value'] / 10\n",
    "    final_merged_df['was_home'] = np.where(final_merged_df['team'] == final_merged_df['team_h'], 1, 0)\n",
    "    return final_merged_df\n",
    "\n",
    "def calculate_rolling_averages(df, window=5):\n",
    "    # Sort the dataframe by team and game\n",
    "    df = df.sort_values(['team', 'game'])\n",
    "    \n",
    "    # Calculate rolling averages for team-level statistics\n",
    "    df['rolling_avg_goals_scored'] = df.groupby('team')['goals_scored'].transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "    df['rolling_avg_goals_conceded'] = df.groupby('team')['goals_conceded'].transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_team_difficulty(df):\n",
    "    # Calculate average difficulty for each team\n",
    "    team_difficulty = df.groupby('team')[['team_h_difficulty', 'team_a_difficulty']].mean().mean(axis=1)\n",
    "    \n",
    "    # Map team difficulty back to the dataframe\n",
    "    df['rolling_team_difficulty'] = df['team'].map(team_difficulty)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def convert_dtypes_for_model(df, model_type):\n",
    "    category_columns = ['position', 'PlayerUniqueID', 'TeamUniqueID', 'TeamUniqueID_oppo', 'game']\n",
    "    for col in category_columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "    return df\n",
    "\n",
    "def calculate_advanced_metrics(df):\n",
    "    df['expected_goals'] = (df['expected_goal_involvements'] * 0.6)\n",
    "    df['expected_assists'] = df['expected_assists']\n",
    "    df['goal_scoring_probability'] = (df['expected_goals'] / df['minutes'].replace(0, 1)) * 100\n",
    "    df['goal_scoring_probability'] = df['goal_scoring_probability'].clip(0, 100)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20adb1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 487us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyle/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Usage\n",
    "\n",
    "# Constants\n",
    "RAW_URL = 'https://raw.githubusercontent.com/vaastav/Fantasy-Premier-League/master/data/2024-25/players_raw.csv'\n",
    "FIXTURES_URL = 'https://raw.githubusercontent.com/vaastav/Fantasy-Premier-League/master/data/2024-25/fixtures.csv'\n",
    "CURRENT_SEASON = '2024-25'\n",
    "CURRENT_GW = 8\n",
    "GAME_ID = f'2024{CURRENT_GW:02d}'\n",
    "\n",
    "# Define required columns based on your model's features\n",
    "required_columns = [\n",
    "    'position', 'xP', 'assists', 'clean_sheets', 'creativity', 'expected_assists',\n",
    "'expected_goal_involvements', 'expected_goals', 'expected_goals_conceded', 'goals_conceded', \n",
    "'goals_scored', 'influence', 'minutes', 'own_goals', 'penalties_missed', 'penalties_saved', \n",
    "'red_cards', 'saves', 'starts', 'threat', 'transfers_balance', 'cost', 'was_home',\n",
    "'yellow_cards', 'PlayerUniqueID', 'TeamUniqueID', 'TeamUniqueID_oppo', 'cumulative_points',\n",
    "'cumulative_minutes', 'ppm', 'rolling_avg_points', 'rolling_avg_goals_scored', \n",
    "'rolling_avg_goals_conceded', 'rolling_team_difficulty', 'game'\n",
    "]\n",
    "\n",
    "\n",
    "# Load the trained neural network model and StandardScaler\n",
    "nn_model = joblib.load(MODEL_NAME)\n",
    "scaler = joblib.load(SCALER_FILENAME)\n",
    "\n",
    "# Load and preprocess data\n",
    "next_gw = load_and_preprocess_data()\n",
    "fixtures = load_and_process_fixtures()\n",
    "final_merged_df = merge_player_and_fixture_data(next_gw, fixtures)\n",
    "\n",
    "upcoming_game_week_data = prepare_for_prediction(final_merged_df, teams, players_unique, required_columns)\n",
    "upcoming_game_week_data = convert_dtypes_for_model(upcoming_game_week_data, 'neural_net')\n",
    "\n",
    "upcoming_game_week_data = calculate_advanced_metrics(upcoming_game_week_data)\n",
    "\n",
    "# Scale the feature data\n",
    "X_upcoming = upcoming_game_week_data[required_columns]\n",
    "X_upcoming_scaled = scaler.transform(X_upcoming)\n",
    "\n",
    "# Make predictions using the neural network model\n",
    "predictions_upcoming = nn_model.predict(X_upcoming_scaled)\n",
    "\n",
    "# Add predictions to the dataframe\n",
    "upcoming_game_week_data['predicted_points'] = predictions_upcoming.flatten()/6\n",
    "#dividing by 6 right now as it looks more realistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de69caef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE ##\n",
    "# Advanced Metrics are a work in progress and don't look all too accurate or correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed22c516",
   "metadata": {},
   "source": [
    "# Select Best Team\n",
    "\n",
    "1. **Team Selection Optimization**:\n",
    "   - **`optimize_team_selection(upcoming_game_week_data, budget=100)`**: This function uses linear programming to select an optimal team of players within a specified budget.\n",
    "     - **Position Constraints**: It defines constraints for player positions (goalkeeper, defenders, midfielders, forwards) and their minimum and maximum counts.\n",
    "     - **Objective Function**: The goal is to maximize the predicted points while adhering to the budget and positional constraints.\n",
    "     - The function returns a selection of players (indicated by a binary array) based on the optimization results.\n",
    "\n",
    "2. **Team Preparation**:\n",
    "   - **`prepare_selected_team(upcoming_game_week_data, selected_players, teams)`**: This function prepares the final selected team by filtering for the chosen players and merging their data with team information.\n",
    "     - It maps position codes to their text representations (e.g., GK, DEF) and formats other relevant columns for output.\n",
    "     - The function returns a DataFrame containing details about the selected players, including their teams, positions, costs, and predicted points.\n",
    "\n",
    "3. **Main Function**:\n",
    "   - **`main(upcoming_game_week_data, teams, budget=100)`**: This function coordinates the selection and preparation of the optimized team by calling the previous two functions. It returns the final team lineup.\n",
    "\n",
    "4. **Execution Block** (commented out): The code includes a section that would execute the `main()` function if the script is run as a standalone program, assuming the required data is already loaded.\n",
    "\n",
    "Overall, the section is designed to optimize fantasy football team selections based on player performance predictions, budget constraints, and positional requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53a68455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_team_selection(upcoming_game_week_data, budget=100):\n",
    "    # Constants and mappings\n",
    "    POSITION_MAPPING = {1: 'GK', 2: 'DEF', 3: 'MID', 4: 'FWD'}\n",
    "    POSITION_COUNTS = {\n",
    "        'GK': (1, 2),\n",
    "        'DEF': (3, 5),\n",
    "        'MID': (3, 5),\n",
    "        'FWD': (1, 3)\n",
    "    }\n",
    "\n",
    "    # Prepare data\n",
    "    player_costs = upcoming_game_week_data['cost'].values\n",
    "    player_points = upcoming_game_week_data['predicted_points'].values\n",
    "    player_positions = upcoming_game_week_data['position'].values\n",
    "    num_players = len(player_costs)\n",
    "\n",
    "    # Objective function: maximize points (minimize negative points)\n",
    "    c = -player_points\n",
    "\n",
    "    # Constraints\n",
    "    A = [player_costs]  # Budget constraint\n",
    "    b = [budget]\n",
    "\n",
    "    # Position constraints\n",
    "    for pos, (min_count, max_count) in POSITION_COUNTS.items():\n",
    "        pos_indicator = (player_positions == [k for k, v in POSITION_MAPPING.items() if v == pos][0]).astype(int)\n",
    "        A.extend([pos_indicator, -pos_indicator])\n",
    "        b.extend([max_count, -min_count])\n",
    "\n",
    "    A = np.array(A)\n",
    "    b = np.array(b)\n",
    "\n",
    "    # Bounds for each player (0 or 1 - either selected or not)\n",
    "    bounds = [(0, 1) for _ in range(num_players)]\n",
    "\n",
    "    # Solve the linear programming problem\n",
    "    result = linprog(c, A_ub=A, b_ub=b, bounds=bounds, method='highs')\n",
    "\n",
    "    # Get the selected players\n",
    "    selected_players = result.x.round().astype(int)\n",
    "\n",
    "    return selected_players\n",
    "\n",
    "def prepare_selected_team(upcoming_game_week_data, selected_players, teams):\n",
    "    # Filter for selected players\n",
    "    selected_team = upcoming_game_week_data[selected_players == 1].copy()\n",
    "\n",
    "    # Merge with team data\n",
    "    current_season_teams = teams[teams['season'] == '2024-25']\n",
    "    selected_team = selected_team.merge(current_season_teams[['TeamUniqueID', 'team_name']], on='TeamUniqueID', how='left')\n",
    "    selected_team = selected_team.merge(current_season_teams[['TeamUniqueID', 'team_name']], left_on='TeamUniqueID_oppo', right_on='TeamUniqueID', how='left', suffixes=('', '_oppo'))\n",
    "    \n",
    "    # Map position codes to text\n",
    "    position_mapping = {1: 'GK', 2: 'DEF', 3: 'MID', 4: 'FWD'}\n",
    "    selected_team['position_txt'] = selected_team['position'].map(position_mapping)\n",
    "\n",
    "    # Prepare output\n",
    "    selected_team['was_home'] = selected_team['was_home'].apply(lambda x: 'H' if x == 1 else 'A')\n",
    "    columns_to_display = ['team_name', 'name', 'position_txt', 'cost', 'predicted_points', 'team_name_oppo',\n",
    "                          'was_home','expected_goals', 'expected_assists', 'goal_scoring_probability']\n",
    "\n",
    "    selected_team = selected_team[columns_to_display].sort_values('predicted_points', ascending=False)\n",
    "\n",
    "    selected_team.rename(columns={'team_name': 'Team', 'name':'Player', 'position_txt':'Position',\n",
    "                                 'cost':'Player Cost', 'predicted_points':'Predicted Gameweek Points',\n",
    "                                 'team_name_oppo':'Opposition Team Name', 'was_home':'Home or Away',\n",
    "                                 'expected_goals':'Xg', 'expected_assists':'Xa',\n",
    "                                 'goal_scoring_probability':'Goal Scoring Probability (%)'}, inplace=True)\n",
    "    return selected_team\n",
    "\n",
    "def main(upcoming_game_week_data, teams, budget=100):\n",
    "    selected_players = optimize_team_selection(upcoming_game_week_data, budget)\n",
    "    optimized_team = prepare_selected_team(upcoming_game_week_data, selected_players, teams)\n",
    "    return optimized_team\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Assuming upcoming_game_week_data and teams are already loaded\n",
    "#     optimized_team = main(upcoming_game_week_data, teams)\n",
    "#     print(optimized_team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ae48846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Team           Player Position  Player Cost  \\\n",
      "12  Liverpool    Mohamed Salah      MID         12.7   \n",
      "5     Chelsea      Cole Palmer      MID         10.8   \n",
      "1     Arsenal      Bukayo Saka      MID         10.1   \n",
      "11  Liverpool        Luis Daz      MID          8.1   \n",
      "0     Arsenal      Kai Havertz      FWD          8.3   \n",
      "4     Chelsea  Nicolas Jackson      FWD          7.9   \n",
      "6     Everton    Dwight McNeil      MID          5.6   \n",
      "3    Brighton    Danny Welbeck      FWD          5.8   \n",
      "2   Brentford   Nathan Collins      DEF          4.5   \n",
      "10  Leicester     James Justin      DEF          4.5   \n",
      "14     Wolves  Rayan At-Nouri      DEF          4.4   \n",
      "7     Ipswich       Leif Davis      DEF          4.5   \n",
      "13    Man Utd      Andr Onana       GK          5.0   \n",
      "8   Leicester        Wout Faes      DEF          4.1   \n",
      "9   Leicester   Mads Hermansen       GK          4.5   \n",
      "\n",
      "    Predicted Gameweek Points Opposition Team Name Home or Away     Xg    Xa  \\\n",
      "12                  11.215724            Liverpool            H  3.222  1.40   \n",
      "5                   11.057892            Liverpool            A  3.516  1.79   \n",
      "1                   10.263695          Bournemouth            A  3.090  3.15   \n",
      "11                   8.002416            Liverpool            H  2.136  0.74   \n",
      "0                    7.628673          Bournemouth            A  2.694  0.20   \n",
      "4                    7.046385            Liverpool            A  3.090  0.23   \n",
      "6                    6.699716              Ipswich            A  2.148  3.06   \n",
      "3                    6.645096            Newcastle            A  2.274  0.29   \n",
      "2                    5.003649              Man Utd            A  0.924  0.30   \n",
      "10                   4.893756          Southampton            A  0.252  0.08   \n",
      "14                   4.227061               Wolves            H  0.888  0.49   \n",
      "7                    4.148990              Ipswich            H  0.996  1.36   \n",
      "13                   3.827693              Man Utd            H  0.000  0.00   \n",
      "8                    3.811959          Southampton            A  0.276  0.07   \n",
      "9                    3.447988          Southampton            A  0.000  0.00   \n",
      "\n",
      "    Goal Scoring Probability (%)  \n",
      "12                      3.734106  \n",
      "5                       4.136471  \n",
      "1                       3.821555  \n",
      "11                      3.352466  \n",
      "0                       2.993333  \n",
      "4                       4.151631  \n",
      "6                       2.417363  \n",
      "3                       2.670805  \n",
      "2                       1.026667  \n",
      "10                      0.280000  \n",
      "14                      1.102128  \n",
      "7                       1.106667  \n",
      "13                      0.000000  \n",
      "8                       0.306667  \n",
      "9                       0.000000  \n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "optimized_team = main(upcoming_game_week_data, teams)\n",
    "print(optimized_team)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62fe4f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the selected team to a CSV file\n",
    "optimized_team.to_csv('selected_team_NN_20242508.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65407441",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create requirements file\n",
    "#!pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
